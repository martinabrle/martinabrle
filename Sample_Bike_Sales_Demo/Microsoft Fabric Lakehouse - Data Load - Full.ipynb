{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2cddb29-1e82-49e4-9c6a-7f491c900b20",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54a41328-c460-41ce-a107-d0c93e041198",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-08T16:38:55.0193402Z",
       "execution_start_time": "2026-02-08T16:38:54.7442963Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "d8574445-5e40-414c-b4d0-979a8f8c912b",
       "queued_time": "2026-02-08T16:38:53.47162Z",
       "session_id": "bbb2dded-1266-4104-933b-c5e028b3a03f",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 4,
       "statement_ids": [
        4
       ]
      },
      "text/plain": [
       "StatementMeta(, bbb2dded-1266-4104-933b-c5e028b3a03f, 4, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, to_date, col\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType, DecimalType\n",
    "import requests\n",
    "import base64\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493eb454-2ffa-4247-803d-e2bc8fa2a3c6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff90094b-1586-4749-9315-34b5a0fa806b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-08T16:38:56.672046Z",
       "execution_start_time": "2026-02-08T16:38:56.3892217Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "e6df8c68-90c3-4d8f-a800-bf77f3da4c49",
       "queued_time": "2026-02-08T16:38:56.3880768Z",
       "session_id": "bbb2dded-1266-4104-933b-c5e028b3a03f",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 5,
       "statement_ids": [
        5
       ]
      },
      "text/plain": [
       "StatementMeta(, bbb2dded-1266-4104-933b-c5e028b3a03f, 5, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "GITHUB_PATH = (\n",
    "    \"https://raw.githubusercontent.com/martinabrle/demo-data/main/Sample_Bike_Sales\"\n",
    ")\n",
    "TENANT_ID=\"\" # only if updating the semantic model from Metadata\n",
    "CLIENT_ID = \"\" # only if updating the semantic model from Metadata\n",
    "CLIENT_SECRET = \"\" # only if updating the semantic model from Metadata\n",
    "WORKSPACE_NAME = \"demo-bike-sales\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868fe05f-dc03-460c-a19e-3d1d1e534c77",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Load Addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7615e87-74a2-4850-8bc6-2e62cc5064e1",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-08T16:40:53.667893Z",
       "execution_start_time": "2026-02-08T16:40:51.9534148Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "c5a0b910-5d07-404d-b4b7-6219433fb310",
       "queued_time": "2026-02-08T16:40:51.9522334Z",
       "session_id": "bbb2dded-1266-4104-933b-c5e028b3a03f",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 7,
       "statement_ids": [
        7
       ]
      },
      "text/plain": [
       "StatementMeta(, bbb2dded-1266-4104-933b-c5e028b3a03f, 7, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Downloading https://raw.githubusercontent.com/martinabrle/demo-data/main/Sample_Bike_Sales/Addresses.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded data file:///mnt/var/hadoop/tmp/nm-secondary-local-dir/usercache/trusted-service-user/appcache/application_1770568009607_0001/spark-4e4ee70a-3207-4b39-b7af-aa00a2e2be54/userFiles-bc845dab-4a20-4f93-8a29-d411e91d3f5f/Addresses.csv\n"
     ]
    }
   ],
   "source": [
    "file_name=\"Addresses.csv\"\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"address_id\", IntegerType(), False, metadata={\"description\": \"Unique identifier for the address\", \"comment\": \"Address identifier\", \"display_name\": \"Address ID\", \"primary_key\": True}),\n",
    "    StructField(\"city\", StringType(), True, metadata={\"description\": \"City of the address\", \"comment\": \"City name\", \"display_name\": \"City\"}),\n",
    "    StructField(\"postal_code\", StringType(), True, metadata={\"description\": \"Postal code of the address\", \"comment\": \"Postal code\", \"display_name\": \"Postal Code\"}),\n",
    "    StructField(\"street\", StringType(), True, metadata={\"description\": \"Street name of the address\", \"comment\": \"Street name\", \"display_name\": \"Street\"}),\n",
    "    StructField(\"building\", StringType(), True, metadata={\"description\": \"Building number of the address\", \"comment\": \"Building number\", \"display_name\": \"Building\"}),\n",
    "    StructField(\"country\", StringType(), True, metadata={\"description\": \"Country of the address\", \"comment\": \"Country code\", \"display_name\": \"Country\"}),\n",
    "    StructField(\"region\", StringType(), True, metadata={\"description\": \"Administrative region, state, or province\", \"comment\": \"Region code\", \"display_name\": \"Region\"}),\n",
    "    StructField(\"address_type\", StringType(), True, metadata={\"description\": \"Classification of the address such as billing, shipping, or home\", \"comment\": \"Address type\", \"display_name\": \"Address Type\"}),\n",
    "    StructField(\"validity_start_date\", StringType(), True, metadata={\"description\": \"Date from which the address is considered valid\", \"comment\": \"Validity start date\", \"display_name\": \"Valid From\"}),\n",
    "    StructField(\"validity_end_date\", StringType(), True, metadata={\"description\": \"Date until which the address is considered valid\", \"comment\": \"Validity end date\", \"display_name\": \"Valid To\"}),\n",
    "    StructField(\"latitude\", DoubleType(), True, metadata={\"description\": \"Geographical latitude coordinate of the address\", \"comment\": \"Latitude\", \"display_name\": \"Latitude\"}),\n",
    "    StructField(\"longitude\", DoubleType(), True, metadata={\"description\": \"Geographical longitude coordinate of the address\", \"comment\": \"Longitude\", \"display_name\": \"Longitude\"})\n",
    "])\n",
    "\n",
    "gh_full_file_name = f\"{GITHUB_PATH}/{file_name}\"\n",
    "display(f\"Downloading {gh_full_file_name}\")\n",
    "\n",
    "sc.addFile(gh_full_file_name)\n",
    "gh_file_name  = 'file://' +SparkFiles.get(file_name)\n",
    "\n",
    "addresses_df = spark.read.csv(path=gh_file_name,header=True,schema=schema)\n",
    "print(f\"Downloaded data {gh_file_name}\")\n",
    "\n",
    "addresses_df = addresses_df.withColumn(\"validity_start_date\", to_date(col(\"validity_start_date\"), \"yyyyMMdd\"))\n",
    "addresses_df = addresses_df.withColumn(\"validity_end_date\", to_date(col(\"validity_end_date\"), \"yyyyMMdd\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359f573d-a590-46ce-9595-83ebf0a77064",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Load Employees and Employee Addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67715cec-ccfa-47c8-902a-48246188e5ee",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-07T15:59:23.503529Z",
       "execution_start_time": "2026-02-07T15:58:27.0001347Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "35acbb68-4999-4214-8420-4b32c7b08f12",
       "queued_time": "2026-02-07T15:58:23.364858Z",
       "session_id": "7e17825f-b6fa-45fb-b49f-f2aecc437fb6",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 10,
       "statement_ids": [
        10
       ]
      },
      "text/plain": [
       "StatementMeta(, 7e17825f-b6fa-45fb-b49f-f2aecc437fb6, 10, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Downloading https://raw.githubusercontent.com/martinabrle/demo-data/main/Sample_Bike_Sales/Employees.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded data file:///mnt/var/hadoop/tmp/nm-secondary-local-dir/usercache/trusted-service-user/appcache/application_1770479201446_0001/spark-d7ad3cdc-6a3b-4e2a-a913-5ebe7243d33f/userFiles-4d17422f-1ff8-428e-bff3-73b493f4847d/Employees.csv\n"
     ]
    }
   ],
   "source": [
    "file_name=\"Employees.csv\"\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"employee_id\", IntegerType(), False, metadata={\"description\": \"Unique identifier of the employee\", \"comment\": \"Employee identifier\", \"display_name\": \"Employee ID\", \"primary_key\": True}),\n",
    "    StructField(\"first_name\", StringType(), True, metadata={\"description\": \"Employee's first name\", \"comment\": \"First name\", \"display_name\": \"First Name\"}),\n",
    "    StructField(\"middle_name\", StringType(), True, metadata={\"description\": \"Employee's middle name\", \"comment\": \"Middle name\", \"display_name\": \"Middle Name\"}),\n",
    "    StructField(\"last_name\", StringType(), True, metadata={\"description\": \"Employee's last name\", \"comment\": \"Last name\", \"display_name\": \"Last Name\"}),\n",
    "    StructField(\"initials\", StringType(), True, metadata={\"description\": \"Employee initials\", \"comment\": \"Initials\", \"display_name\": \"Initials\"}),\n",
    "    StructField(\"country\", StringType(), True, metadata={\"description\": \"Country associated with the employee\", \"comment\": \"Country\", \"display_name\": \"Country\"}),\n",
    "    StructField(\"gender\", StringType(), True, metadata={\"description\": \"Gender of the employee\", \"comment\": \"Gender\", \"display_name\": \"Gender\"}),\n",
    "    StructField(\"language\", StringType(), True, metadata={\"description\": \"Preferred language of the employee\", \"comment\": \"Language\", \"display_name\": \"Language\"}),\n",
    "    StructField(\"phone_no\", StringType(), True, metadata={\"description\": \"Employee phone number\", \"comment\": \"Phone number\", \"display_name\": \"Phone Number\"}),\n",
    "    StructField(\"email\", StringType(), True, metadata={\"description\": \"Employee email address\", \"comment\": \"Email address\", \"display_name\": \"Email\"}),\n",
    "    StructField(\"login_name\", StringType(), True, metadata={\"description\": \"Login name used by the employee\", \"comment\": \"Login name\", \"display_name\": \"Login Name\"}),\n",
    "    StructField(\"address_id\", IntegerType(), True, metadata={\"description\": \"Identifier of the employee's address\", \"comment\": \"Address identifier\", \"display_name\": \"Address ID\"}),\n",
    "    StructField(\"validity_start_date\", StringType(), True, metadata={\"description\": \"Date from which the employee record is valid\", \"comment\": \"Validity start date\", \"display_name\": \"Valid From\"}),\n",
    "    StructField(\"validity_end_date\", StringType(), True, metadata={\"description\": \"Date until which the employee record is valid\", \"comment\": \"Validity end date\", \"display_name\": \"Valid To\"})\n",
    "])\n",
    "\n",
    "gh_full_file_name = f\"{GITHUB_PATH}/{file_name}\"\n",
    "display(f\"Downloading {gh_full_file_name}\")\n",
    "\n",
    "sc.addFile(gh_full_file_name)\n",
    "gh_file_name  = 'file://' +SparkFiles.get(file_name)\n",
    "\n",
    "employees_df = spark.read.csv(path=gh_file_name,header=True,schema=schema)\n",
    "print(f\"Downloaded data {gh_file_name}\")\n",
    "\n",
    "employees_df = employees_df.withColumn(\"validity_start_date\", to_date(col(\"validity_start_date\"), \"yyyyMMdd\"))\n",
    "employees_df = employees_df.withColumn(\"validity_end_date\", to_date(col(\"validity_end_date\"), \"yyyyMMdd\"))\n",
    "\n",
    "employees_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"employees\")\n",
    "\n",
    "df_employee_addresses = addresses_df.join(\n",
    "    employees_df,\n",
    "    on=\"address_id\",\n",
    "    how=\"left_semi\"\n",
    ")\n",
    "\n",
    "df_employee_addresses.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"employee_addresses\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8f2c98-b767-45bf-baa9-ff5a74922013",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Load Customers and Customer Addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46d0f03-e2aa-48ed-b024-b85fe47146dd",
   "metadata": {
    "advisor": {
     "adviceMetadata": "{\"artifactId\":\"621b7fe4-1488-49ca-917b-6a36e2774b21\",\"activityId\":\"bbb2dded-1266-4104-933b-c5e028b3a03f\",\"applicationId\":\"application_1770568009607_0001\",\"jobGroupId\":\"8\",\"advices\":{\"error\":1}}"
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-08T16:42:21.5656187Z",
       "execution_start_time": "2026-02-08T16:41:31.4991743Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "07671c6f-02b5-4ff4-9a89-62a245bc6168",
       "queued_time": "2026-02-08T16:41:31.4978709Z",
       "session_id": "bbb2dded-1266-4104-933b-c5e028b3a03f",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 8,
       "statement_ids": [
        8
       ]
      },
      "text/plain": [
       "StatementMeta(, bbb2dded-1266-4104-933b-c5e028b3a03f, 8, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Downloading https://raw.githubusercontent.com/martinabrle/demo-data/main/Sample_Bike_Sales/Customers.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded data file:///mnt/var/hadoop/tmp/nm-secondary-local-dir/usercache/trusted-service-user/appcache/application_1770568009607_0001/spark-4e4ee70a-3207-4b39-b7af-aa00a2e2be54/userFiles-bc845dab-4a20-4f93-8a29-d411e91d3f5f/Customers.csv\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o12143.saveAsTable.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 45.0 failed 4 times, most recent failure: Lost task 0.3 in stage 45.0 (TID 196) (vm-a4082385 executor 2): org.apache.spark.SparkFileNotFoundException: File file:/mnt/var/hadoop/tmp/nm-secondary-local-dir/usercache/trusted-service-user/appcache/application_1770568009607_0001/spark-4e4ee70a-3207-4b39-b7af-aa00a2e2be54/userFiles-bc845dab-4a20-4f93-8a29-d411e91d3f5f/Customers.csv does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:783)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:285)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:348)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:188)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:413)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:900)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:900)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:57)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:368)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:332)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:636)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:639)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3111)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3047)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3046)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3046)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3318)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3249)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3238)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1037)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2580)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2601)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2645)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1056)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:411)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1055)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectIterator(SparkPlan.scala:480)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:149)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:354)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:349)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/mnt/var/hadoop/tmp/nm-secondary-local-dir/usercache/trusted-service-user/appcache/application_1770568009607_0001/spark-4e4ee70a-3207-4b39-b7af-aa00a2e2be54/userFiles-bc845dab-4a20-4f93-8a29-d411e91d3f5f/Customers.csv does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:783)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:285)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:348)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:188)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:413)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:900)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:900)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:57)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:368)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:332)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:636)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:639)\n\t... 3 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 39\u001b[0m\n\u001b[1;32m     31\u001b[0m customers_df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msaveAsTable(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustomers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m df_customer_addresses \u001b[38;5;241m=\u001b[39m addresses_df\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m     34\u001b[0m     customers_df,\n\u001b[1;32m     35\u001b[0m     on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maddress_id\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     36\u001b[0m     how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft_semi\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     37\u001b[0m )\n\u001b[0;32m---> 39\u001b[0m df_customer_addresses\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msaveAsTable(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustomer_addresses\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py:1586\u001b[0m, in \u001b[0;36mDataFrameWriter.saveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1585\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m-> 1586\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msaveAsTable(name)\n",
      "File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o12143.saveAsTable.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 45.0 failed 4 times, most recent failure: Lost task 0.3 in stage 45.0 (TID 196) (vm-a4082385 executor 2): org.apache.spark.SparkFileNotFoundException: File file:/mnt/var/hadoop/tmp/nm-secondary-local-dir/usercache/trusted-service-user/appcache/application_1770568009607_0001/spark-4e4ee70a-3207-4b39-b7af-aa00a2e2be54/userFiles-bc845dab-4a20-4f93-8a29-d411e91d3f5f/Customers.csv does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:783)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:285)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:348)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:188)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:413)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:900)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:900)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:57)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:368)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:332)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:636)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:639)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3111)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3047)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3046)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3046)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3318)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3249)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3238)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1037)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2580)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2601)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2645)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1056)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:411)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1055)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectIterator(SparkPlan.scala:480)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:149)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:354)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:349)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/mnt/var/hadoop/tmp/nm-secondary-local-dir/usercache/trusted-service-user/appcache/application_1770568009607_0001/spark-4e4ee70a-3207-4b39-b7af-aa00a2e2be54/userFiles-bc845dab-4a20-4f93-8a29-d411e91d3f5f/Customers.csv does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:783)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:285)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:348)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:188)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:413)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:900)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:900)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:57)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:368)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:332)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:636)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:639)\n\t... 3 more\n"
     ]
    }
   ],
   "source": [
    "file_name=\"Customers.csv\"\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), False, metadata={\"description\": \"Unique identifier of the customer\", \"comment\": \"Customer identifier\", \"display_name\": \"Customer ID\", \"primary_key\": True}),\n",
    "    StructField(\"email\", StringType(), True, metadata={\"description\": \"Customer email address\", \"comment\": \"Email address\", \"display_name\": \"Email\"}),\n",
    "    StructField(\"phone_no\", StringType(), True, metadata={\"description\": \"Customer phone number\", \"comment\": \"Phone number\", \"display_name\": \"Phone Number\"}),\n",
    "    StructField(\"fax_no\", StringType(), True, metadata={\"description\": \"Customer fax number\", \"comment\": \"Fax number\", \"display_name\": \"Fax Number\"}),\n",
    "    StructField(\"url\", StringType(), True, metadata={\"description\": \"Customer website or URL\", \"comment\": \"Website URL\", \"display_name\": \"Website\"}),\n",
    "    StructField(\"address_id\", IntegerType(), True, metadata={\"description\": \"Identifier of the customer's address\", \"comment\": \"Address identifier\", \"display_name\": \"Address ID\"}),\n",
    "    StructField(\"company_name\", StringType(), True, metadata={\"description\": \"Legal or registered name of the company\", \"comment\": \"Company name\", \"display_name\": \"Customer\"}),\n",
    "    StructField(\"legal_form\", StringType(), True, metadata={\"description\": \"Legal form of the company, such as Ltd or Inc\", \"comment\": \"Legal form\", \"display_name\": \"Legal Form\"}),\n",
    "    StructField(\"created_by\", IntegerType(), True, metadata={\"description\": \"Identifier of the user who created the customer record\", \"comment\": \"Created by\", \"display_name\": \"Created By\"}),\n",
    "    StructField(\"created_date\", StringType(), True, metadata={\"description\": \"Date when the customer record was created\", \"comment\": \"Created date\", \"display_name\": \"Created Date\"}),\n",
    "    StructField(\"modified_by\", IntegerType(), True, metadata={\"description\": \"Identifier of the user who last modified the customer record\", \"comment\": \"Modified by\", \"display_name\": \"Modified By\"}),\n",
    "    StructField(\"modified_date\", StringType(), True, metadata={\"description\": \"Date when the customer record was last modified\", \"comment\": \"Modified date\", \"display_name\": \"Modified Date\"}),\n",
    "    StructField(\"currency\", StringType(), True, metadata={\"description\": \"Default currency associated with the customer\", \"comment\": \"Currency\", \"display_name\": \"Currency\" })\n",
    "])\n",
    "\n",
    "gh_full_file_name = f\"{GITHUB_PATH}/{file_name}\"\n",
    "display(f\"Downloading {gh_full_file_name}\")\n",
    "\n",
    "sc.addFile(gh_full_file_name)\n",
    "gh_file_name  = 'file://' +SparkFiles.get(file_name)\n",
    "\n",
    "customers_df = spark.read.csv(path=gh_file_name,header=True,schema=schema)\n",
    "print(f\"Downloaded data {gh_file_name}\")\n",
    "\n",
    "customers_df = customers_df.withColumn(\"created_date\", to_date(col(\"created_date\"), \"yyyyMMdd\"))\n",
    "customers_df = customers_df.withColumn(\"modified_date\", to_date(col(\"modified_date\"), \"yyyyMMdd\"))\n",
    "\n",
    "customers_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"customers\")\n",
    "\n",
    "df_customer_addresses = addresses_df.join(\n",
    "    customers_df,\n",
    "    on=\"address_id\",\n",
    "    how=\"left_semi\"\n",
    ")\n",
    "\n",
    "df_customer_addresses.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"customer_addresses\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b678fe-d120-4feb-9f81-9c2c98660195",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Load Vendors and Vendor Addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e2413a-b4f6-4290-b489-2be2b20a6849",
   "metadata": {
    "advisor": {
     "adviceMetadata": "{\"artifactId\":\"621b7fe4-1488-49ca-917b-6a36e2774b21\",\"activityId\":\"cbc235ee-e949-4a4f-bc4e-8f3159d74f2d\",\"applicationId\":\"application_1770492411878_0001\",\"jobGroupId\":\"10\",\"advices\":{\"error\":1}}"
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-07T19:52:07.8412667Z",
       "execution_start_time": "2026-02-07T19:51:45.367482Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "a08d29e6-dc50-4f51-abdb-ab4242951c94",
       "queued_time": "2026-02-07T19:51:45.366268Z",
       "session_id": "cbc235ee-e949-4a4f-bc4e-8f3159d74f2d",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 10,
       "statement_ids": [
        10
       ]
      },
      "text/plain": [
       "StatementMeta(, cbc235ee-e949-4a4f-bc4e-8f3159d74f2d, 10, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Downloading https://raw.githubusercontent.com/martinabrle/demo-data/main/Sample_Bike_Sales/Vendors.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded data file:///mnt/var/hadoop/tmp/nm-local-dir/usercache/trusted-service-user/appcache/application_1770492411878_0001/spark-400262e1-c607-4b54-bca3-63b28d3d28fd/userFiles-fcb2385f-723c-40b8-9274-532143b2746f/Vendors.csv\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o14943.saveAsTable.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 60.0 failed 4 times, most recent failure: Lost task 0.3 in stage 60.0 (TID 221) (vm-22754175 executor 3): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to abfss://1a7264cf-5a08-47ab-aa00-41a6f005f2d3@onelake.dfs.fabric.microsoft.com/6f4189fd-1e9b-4b03-b188-66b54ecf0c9b/Tables/vendors.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:777)\n\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.executeTask(DeltaFileFormatWriter.scala:621)\n\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.$anonfun$executeWrite$4(DeltaFileFormatWriter.scala:387)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:636)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:639)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/mnt/var/hadoop/tmp/nm-local-dir/usercache/trusted-service-user/appcache/application_1770492411878_0001/spark-400262e1-c607-4b54-bca3-63b28d3d28fd/userFiles-fcb2385f-723c-40b8-9274-532143b2746f/Vendors.csv does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:783)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:285)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:348)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:188)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:121)\n\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.$anonfun$executeTask$3(DeltaFileFormatWriter.scala:603)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.executeTask(DeltaFileFormatWriter.scala:611)\n\t... 12 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3111)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3047)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3046)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3046)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3318)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3249)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3238)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1037)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2580)\n\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.$anonfun$executeWrite$1(DeltaFileFormatWriter.scala:384)\n\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.writeAndCommit(DeltaFileFormatWriter.scala:418)\n\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.executeWrite(DeltaFileFormatWriter.scala:315)\n\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.write(DeltaFileFormatWriter.scala:283)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.callDeltaFileFormatWriter$1(TransactionalWrite.scala:582)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.$anonfun$tryWriteFiles$5(TransactionalWrite.scala:593)\n\tat scala.Option.flatMap(Option.scala:271)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.$anonfun$tryWriteFiles$3(TransactionalWrite.scala:593)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$2(SQLExecution.scala:274)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:331)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:270)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:955)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:261)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.tryWriteFiles(TransactionalWrite.scala:528)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:412)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:386)\n\tat org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:151)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:383)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:373)\n\tat org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:151)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:258)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:254)\n\tat org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:151)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:243)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:240)\n\tat org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:151)\n\tat org.apache.spark.sql.delta.commands.WriteIntoDelta.writeFiles(WriteIntoDelta.scala:362)\n\tat org.apache.spark.sql.delta.commands.WriteIntoDelta.writeAndReturnCommitData(WriteIntoDelta.scala:310)\n\tat org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.doDeltaWrite$1(CreateDeltaTableCommand.scala:258)\n\tat org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.handleCreateTableAsSelect(CreateDeltaTableCommand.scala:285)\n\tat org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.handleCommit(CreateDeltaTableCommand.scala:158)\n\tat org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.$anonfun$run$4(CreateDeltaTableCommand.scala:118)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:169)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:167)\n\tat org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:59)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:137)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation(SynapseLoggingShim.scala:111)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation$(SynapseLoggingShim.scala:93)\n\tat org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:59)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:136)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:126)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:116)\n\tat org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:59)\n\tat org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:118)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:214)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:169)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:167)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:66)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.org$apache$spark$sql$delta$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:102)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog$StagedDeltaTableV2.$anonfun$commitStagedChanges$1(DeltaCatalog.scala:601)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:169)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:167)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:66)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:587)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:595)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable(WriteToDataSourceV2Exec.scala:588)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable$(WriteToDataSourceV2Exec.scala:582)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:196)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:231)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:235)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$2(SQLExecution.scala:274)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:331)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:270)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:955)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:261)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:235)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:223)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:36)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:278)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:274)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:36)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:36)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:223)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:207)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:201)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:283)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:915)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:675)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:604)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to abfss://1a7264cf-5a08-47ab-aa00-41a6f005f2d3@onelake.dfs.fabric.microsoft.com/6f4189fd-1e9b-4b03-b188-66b54ecf0c9b/Tables/vendors.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:777)\n\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.executeTask(DeltaFileFormatWriter.scala:621)\n\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.$anonfun$executeWrite$4(DeltaFileFormatWriter.scala:387)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:636)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:639)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/mnt/var/hadoop/tmp/nm-local-dir/usercache/trusted-service-user/appcache/application_1770492411878_0001/spark-400262e1-c607-4b54-bca3-63b28d3d28fd/userFiles-fcb2385f-723c-40b8-9274-532143b2746f/Vendors.csv does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:783)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:285)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:348)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:188)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:121)\n\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.$anonfun$executeTask$3(DeltaFileFormatWriter.scala:603)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.executeTask(DeltaFileFormatWriter.scala:611)\n\t... 12 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m vendors_df \u001b[38;5;241m=\u001b[39m vendors_df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreated_date\u001b[39m\u001b[38;5;124m\"\u001b[39m, to_date(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreated_date\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myyyyMMdd\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     29\u001b[0m vendors_df \u001b[38;5;241m=\u001b[39m vendors_df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodified_date\u001b[39m\u001b[38;5;124m\"\u001b[39m, to_date(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodified_date\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myyyyMMdd\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m---> 31\u001b[0m vendors_df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msaveAsTable(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvendors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m df_vendor_addresses \u001b[38;5;241m=\u001b[39m addresses_df\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m     34\u001b[0m     vendors_df,\n\u001b[1;32m     35\u001b[0m     on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maddress_id\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     36\u001b[0m     how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft_semi\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     39\u001b[0m df_vendor_addresses\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msaveAsTable(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvendor_addresses\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py:1586\u001b[0m, in \u001b[0;36mDataFrameWriter.saveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1585\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m-> 1586\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msaveAsTable(name)\n",
      "File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o14943.saveAsTable.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 60.0 failed 4 times, most recent failure: Lost task 0.3 in stage 60.0 (TID 221) (vm-22754175 executor 3): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to abfss://1a7264cf-5a08-47ab-aa00-41a6f005f2d3@onelake.dfs.fabric.microsoft.com/6f4189fd-1e9b-4b03-b188-66b54ecf0c9b/Tables/vendors.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:777)\n\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.executeTask(DeltaFileFormatWriter.scala:621)\n\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.$anonfun$executeWrite$4(DeltaFileFormatWriter.scala:387)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:636)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:639)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/mnt/var/hadoop/tmp/nm-local-dir/usercache/trusted-service-user/appcache/application_1770492411878_0001/spark-400262e1-c607-4b54-bca3-63b28d3d28fd/userFiles-fcb2385f-723c-40b8-9274-532143b2746f/Vendors.csv does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:783)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:285)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:348)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:188)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:121)\n\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.$anonfun$executeTask$3(DeltaFileFormatWriter.scala:603)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.executeTask(DeltaFileFormatWriter.scala:611)\n\t... 12 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3111)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3047)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3046)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3046)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3318)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3249)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3238)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1037)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2580)\n\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.$anonfun$executeWrite$1(DeltaFileFormatWriter.scala:384)\n\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.writeAndCommit(DeltaFileFormatWriter.scala:418)\n\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.executeWrite(DeltaFileFormatWriter.scala:315)\n\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.write(DeltaFileFormatWriter.scala:283)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.callDeltaFileFormatWriter$1(TransactionalWrite.scala:582)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.$anonfun$tryWriteFiles$5(TransactionalWrite.scala:593)\n\tat scala.Option.flatMap(Option.scala:271)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.$anonfun$tryWriteFiles$3(TransactionalWrite.scala:593)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$2(SQLExecution.scala:274)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:331)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:270)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:955)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:261)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.tryWriteFiles(TransactionalWrite.scala:528)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:412)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:386)\n\tat org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:151)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:383)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:373)\n\tat org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:151)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:258)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:254)\n\tat org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:151)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:243)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:240)\n\tat org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:151)\n\tat org.apache.spark.sql.delta.commands.WriteIntoDelta.writeFiles(WriteIntoDelta.scala:362)\n\tat org.apache.spark.sql.delta.commands.WriteIntoDelta.writeAndReturnCommitData(WriteIntoDelta.scala:310)\n\tat org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.doDeltaWrite$1(CreateDeltaTableCommand.scala:258)\n\tat org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.handleCreateTableAsSelect(CreateDeltaTableCommand.scala:285)\n\tat org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.handleCommit(CreateDeltaTableCommand.scala:158)\n\tat org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.$anonfun$run$4(CreateDeltaTableCommand.scala:118)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:169)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:167)\n\tat org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:59)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:137)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation(SynapseLoggingShim.scala:111)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation$(SynapseLoggingShim.scala:93)\n\tat org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:59)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:136)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:126)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:116)\n\tat org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:59)\n\tat org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:118)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:214)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:169)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:167)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:66)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.org$apache$spark$sql$delta$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:102)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog$StagedDeltaTableV2.$anonfun$commitStagedChanges$1(DeltaCatalog.scala:601)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:169)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:167)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:66)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:587)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:595)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable(WriteToDataSourceV2Exec.scala:588)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable$(WriteToDataSourceV2Exec.scala:582)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:196)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:231)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:235)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$2(SQLExecution.scala:274)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:331)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:270)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:955)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:261)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:235)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:223)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:36)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:278)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:274)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:36)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:36)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:223)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:207)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:201)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:283)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:915)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:675)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:604)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to abfss://1a7264cf-5a08-47ab-aa00-41a6f005f2d3@onelake.dfs.fabric.microsoft.com/6f4189fd-1e9b-4b03-b188-66b54ecf0c9b/Tables/vendors.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:777)\n\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.executeTask(DeltaFileFormatWriter.scala:621)\n\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.$anonfun$executeWrite$4(DeltaFileFormatWriter.scala:387)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:636)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:639)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/mnt/var/hadoop/tmp/nm-local-dir/usercache/trusted-service-user/appcache/application_1770492411878_0001/spark-400262e1-c607-4b54-bca3-63b28d3d28fd/userFiles-fcb2385f-723c-40b8-9274-532143b2746f/Vendors.csv does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:783)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:285)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:348)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:188)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:121)\n\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.$anonfun$executeTask$3(DeltaFileFormatWriter.scala:603)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.executeTask(DeltaFileFormatWriter.scala:611)\n\t... 12 more\n"
     ]
    }
   ],
   "source": [
    "file_name=\"Vendors.csv\"\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"vendor_id\", IntegerType(), False, metadata={\"description\": \"Unique identifier of the vendor\", \"comment\": \"Vendor identifier\", \"display_name\": \"Vendor ID\", \"primary_key\": True}),\n",
    "    StructField(\"email\", StringType(), True, metadata={\"description\": \"Vendor email address\", \"comment\": \"Email address\", \"display_name\": \"Email\"}),\n",
    "    StructField(\"phone_no\", StringType(), True, metadata={\"description\": \"Vendor phone number\", \"comment\": \"Phone number\", \"display_name\": \"Phone Number\"}),\n",
    "    StructField(\"fax_no\", StringType(), True, metadata={\"description\": \"Vendor fax number\", \"comment\": \"Fax number\", \"display_name\": \"Fax Number\"}),\n",
    "    StructField(\"url\", StringType(), True, metadata={\"description\": \"Vendor website or URL\", \"comment\": \"Website URL\", \"display_name\": \"Website\"}),\n",
    "    StructField(\"address_id\", IntegerType(), True, metadata={\"description\": \"Identifier for the vendor's address\", \"comment\": \"Address identifier\", \"display_name\": \"Address ID\"}),\n",
    "    StructField(\"company_name\", StringType(), True, metadata={\"description\": \"Legal or registered name of the vendor company\", \"comment\": \"Company name\", \"display_name\": \"Vendor\"}),\n",
    "    StructField(\"legal_form\", StringType(), True, metadata={\"description\": \"Legal form of the vendor company, such as Ltd or Inc\", \"comment\": \"Legal form\", \"display_name\": \"Legal Form\"}),\n",
    "    StructField(\"created_by\", IntegerType(), True, metadata={\"description\": \"Identifier of the user who created the vendor record\", \"comment\": \"Created by\", \"display_name\": \"Created By\"}),\n",
    "    StructField(\"created_date\", StringType(), True, metadata={\"description\": \"Date when the vendor record was created\", \"comment\": \"Created date\", \"display_name\": \"Created Date\"}),\n",
    "    StructField(\"modified_by\", IntegerType(), True, metadata={\"description\": \"Identifier of the user who last modified the vendor record\", \"comment\": \"Modified by\", \"display_name\": \"Modified By\"}),\n",
    "    StructField(\"modified_date\", StringType(), True, metadata={\"description\": \"Date when the vendor record was last modified\", \"comment\": \"Modified date\", \"display_name\": \"Modified Date\"}),\n",
    "    StructField(\"currency\", StringType(), True, metadata={\"description\": \"Default currency associated with the vendor\", \"comment\": \"Currency\", \"display_name\": \"Currency\"})\n",
    "])\n",
    "\n",
    "gh_full_file_name = f\"{GITHUB_PATH}/{file_name}\"\n",
    "display(f\"Downloading {gh_full_file_name}\")\n",
    "\n",
    "sc.addFile(gh_full_file_name)\n",
    "gh_file_name  = 'file://' +SparkFiles.get(file_name)\n",
    "\n",
    "vendors_df = spark.read.csv(path=gh_file_name,header=True,schema=schema)\n",
    "print(f\"Downloaded data {gh_file_name}\")\n",
    "\n",
    "vendors_df = vendors_df.withColumn(\"created_date\", to_date(col(\"created_date\"), \"yyyyMMdd\"))\n",
    "vendors_df = vendors_df.withColumn(\"modified_date\", to_date(col(\"modified_date\"), \"yyyyMMdd\"))\n",
    "\n",
    "vendors_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"vendors\")\n",
    "\n",
    "df_vendor_addresses = addresses_df.join(\n",
    "    vendors_df,\n",
    "    on=\"address_id\",\n",
    "    how=\"left_semi\"\n",
    ")\n",
    "\n",
    "df_vendor_addresses.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"vendor_addresses\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f24cd6-3978-4d0f-9863-505295ca6e1c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Load Product Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f1934d9-be86-4c53-adfc-a238ad14c709",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-07T19:49:05.3569845Z",
       "execution_start_time": "2026-02-07T19:48:45.2151388Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "1b5f559b-5f3e-47f5-880b-40929a392d44",
       "queued_time": "2026-02-07T19:48:45.2138865Z",
       "session_id": "cbc235ee-e949-4a4f-bc4e-8f3159d74f2d",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 8,
       "statement_ids": [
        8
       ]
      },
      "text/plain": [
       "StatementMeta(, cbc235ee-e949-4a4f-bc4e-8f3159d74f2d, 8, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Downloading https://raw.githubusercontent.com/martinabrle/demo-data/main/Sample_Bike_Sales/ProductCategories.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded data file:///mnt/var/hadoop/tmp/nm-local-dir/usercache/trusted-service-user/appcache/application_1770492411878_0001/spark-400262e1-c607-4b54-bca3-63b28d3d28fd/userFiles-fcb2385f-723c-40b8-9274-532143b2746f/ProductCategories.csv\n"
     ]
    }
   ],
   "source": [
    "file_name=\"ProductCategories.csv\"\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"product_category_id\", StringType(), False, metadata={\"description\": \"Unique identifier of the product category\", \"comment\": \"Product category ID\", \"display_name\": \"Product Category ID\", \"primary_key\": True}),\n",
    "    StructField(\"created_by\", IntegerType(), True, metadata={\"description\": \"Identifier of the user who created the product category\", \"comment\": \"Created by\", \"display_name\": \"Created By\"}),\n",
    "    StructField(\"created_date\", StringType(), True, metadata={\"description\": \"Date when the product category record was created\", \"comment\": \"Created date\", \"display_name\": \"Created Date\"})\n",
    "])\n",
    "# schema = StructType([\n",
    "#     StructField(\"product_category_id\", StringType(), False),\n",
    "#     StructField(\"created_by\", IntegerType(), True),\n",
    "#     StructField(\"created_date\", StringType(), True)\n",
    "# ])\n",
    "\n",
    "gh_full_file_name = f\"{GITHUB_PATH}/{file_name}\"\n",
    "display(f\"Downloading {gh_full_file_name}\")\n",
    "\n",
    "sc.addFile(gh_full_file_name)\n",
    "gh_file_name  = 'file://' +SparkFiles.get(file_name)\n",
    "\n",
    "product_categories_df = spark.read.csv(path=gh_file_name,header=True,schema=schema)\n",
    "print(f\"Downloaded data {gh_file_name}\")\n",
    "\n",
    "product_categories_df = product_categories_df.withColumn(\"created_date\", to_date(col(\"created_date\"), \"yyyyMMdd\"))\n",
    "\n",
    "product_categories_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"product_categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400732c3-ddf7-4728-a202-98942b9b8654",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Load Product Category Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d934ca1b-410d-4931-a05c-e85cc04fa61e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-08T16:39:48.86158Z",
       "execution_start_time": "2026-02-08T16:39:13.7914213Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "02ad420f-ce44-41c4-ab1d-5856b643eb29",
       "queued_time": "2026-02-08T16:39:13.7903152Z",
       "session_id": "bbb2dded-1266-4104-933b-c5e028b3a03f",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 6,
       "statement_ids": [
        6
       ]
      },
      "text/plain": [
       "StatementMeta(, bbb2dded-1266-4104-933b-c5e028b3a03f, 6, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Downloading https://raw.githubusercontent.com/martinabrle/demo-data/main/Sample_Bike_Sales/ProductCategoryTexts.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded data file:///mnt/var/hadoop/tmp/nm-secondary-local-dir/usercache/trusted-service-user/appcache/application_1770568009607_0001/spark-4e4ee70a-3207-4b39-b7af-aa00a2e2be54/userFiles-bc845dab-4a20-4f93-8a29-d411e91d3f5f/ProductCategoryTexts.csv\n"
     ]
    }
   ],
   "source": [
    "file_name=\"ProductCategoryTexts.csv\"\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"product_category_id\", StringType(), False, metadata={\"description\": \"Unique identifier of the product category\", \"comment\": \"Product category ID\", \"display_name\": \"Product Category ID\"}),\n",
    "    StructField(\"language\", StringType(), True, metadata={\"description\": \"Language code for the product category description\", \"comment\": \"Language\", \"display_name\": \"Language\"}),\n",
    "    StructField(\"short_description\", StringType(), True, metadata={\"description\": \"Category\", \"comment\": \"Short description\", \"display_name\": \"Short Description\"}),\n",
    "    StructField(\"medium_description\", StringType(), True, metadata={\"description\": \"Medium-length description of the product category\", \"comment\": \"Medium description\", \"display_name\": \"Medium Description\"}),\n",
    "    StructField(\"long_description\", StringType(), True, metadata={\"description\": \"Long, detailed description of the product category\", \"comment\": \"Long description\", \"display_name\": \"Long Description\"})\n",
    "])\n",
    "\n",
    "gh_full_file_name = f\"{GITHUB_PATH}/{file_name}\"\n",
    "display(f\"Downloading {gh_full_file_name}\")\n",
    "\n",
    "sc.addFile(gh_full_file_name)\n",
    "gh_file_name  = 'file://' +SparkFiles.get(file_name)\n",
    "\n",
    "product_category_texts_df = spark.read.csv(path=gh_file_name,header=True,schema=schema)\n",
    "print(f\"Downloaded data {gh_file_name}\")\n",
    "\n",
    "product_category_texts_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"product_category_texts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe4f4ee-1fcb-4ce4-b878-3c2e738a7a44",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Load Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98adf672-0763-4bff-a98e-8e4a45004300",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-08T09:57:04.0691938Z",
       "execution_start_time": "2026-02-08T09:56:22.5186432Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "3ab685fd-3d80-4c41-a706-ff9fe3371210",
       "queued_time": "2026-02-08T09:56:22.5175255Z",
       "session_id": "0958bb78-4d2e-4bb0-8a5e-e96cb671914c",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 6,
       "statement_ids": [
        6
       ]
      },
      "text/plain": [
       "StatementMeta(, 0958bb78-4d2e-4bb0-8a5e-e96cb671914c, 6, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Downloading https://raw.githubusercontent.com/martinabrle/demo-data/main/Sample_Bike_Sales/Products.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded data file:///mnt/var/hadoop/tmp/nm-secondary-local-dir/usercache/trusted-service-user/appcache/application_1770543756420_0001/spark-ba0537c7-9332-4ab2-a377-5e54be62443f/userFiles-d84a831e-0708-41c7-b574-f77b3fc6db24/Products.csv\n"
     ]
    }
   ],
   "source": [
    "file_name=\"Products.csv\"\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"product_id\", StringType(), False, metadata={\"description\": \"Unique identifier of the product\", \"comment\": \"Product ID\", \"display_name\": \"Product ID\", \"primary_key\": True}),\n",
    "    StructField(\"type_code\", StringType(), True, metadata={\"description\": \"Type code of the product\", \"comment\": \"Type code\", \"display_name\": \"Type Code\"}),\n",
    "    StructField(\"product_category_id\", StringType(), True, metadata={\"description\": \"Identifier of the product category\", \"comment\": \"Product category ID\", \"display_name\": \"Product Category ID\"}),\n",
    "    StructField(\"created_by\", IntegerType(), True, metadata={\"description\": \"Identifier of the user who created the product record\", \"comment\": \"Created by\", \"display_name\": \"Created By\"}),\n",
    "    StructField(\"created_date\", StringType(), True, metadata={\"description\": \"Date when the product record was created\", \"comment\": \"Created date\", \"display_name\": \"Created Date\"}),\n",
    "    StructField(\"modified_by\", IntegerType(), True, metadata={\"description\": \"Identifier of the user who last modified the product record\", \"comment\": \"Modified by\", \"display_name\": \"Modified By\"}),\n",
    "    StructField(\"modified_date\", StringType(), True, metadata={\"description\": \"Date when the product record was last modified\", \"comment\": \"Modified date\", \"display_name\": \"Modified Date\"}),\n",
    "    StructField(\"vendor_id\", IntegerType(), True, metadata={\"description\": \"Identifier of the vendor supplying the product\", \"comment\": \"Vendor ID\", \"display_name\": \"Vendor ID\"}),\n",
    "    StructField(\"tax_tariff_code\", StringType(), True, metadata={\"description\": \"Tax tariff or classification code for the product\", \"comment\": \"Tax tariff code\", \"display_name\": \"Tax Tariff Code\"}),\n",
    "    StructField(\"quantity_unit\", StringType(), True, metadata={\"description\": \"Unit of measure for the product quantity\", \"comment\": \"Quantity unit\", \"display_name\": \"Quantity Unit\"}),\n",
    "    StructField(\"weight_measure\", DoubleType(), True, metadata={\"description\": \"Weight of the product\", \"comment\": \"Weight measure\", \"display_name\": \"Weight\"}),\n",
    "    StructField(\"weight_unit\", StringType(), True, metadata={\"description\": \"Unit of weight\", \"comment\": \"Weight unit\", \"display_name\": \"Weight Unit\"}),\n",
    "    StructField(\"currency\", StringType(), True, metadata={\"description\": \"Currency for the product pricing\", \"comment\": \"Currency\", \"display_name\": \"Currency\"}),\n",
    "    StructField(\"price\", DecimalType(), True, metadata={\"description\": \"Price of the product\", \"comment\": \"Price\", \"display_name\": \"Price\"}),\n",
    "    StructField(\"width\", DoubleType(), True, metadata={\"description\": \"Width of the product\", \"comment\": \"Width\", \"display_name\": \"Width\"}),\n",
    "    StructField(\"depth\", DoubleType(), True, metadata={\"description\": \"Depth of the product\", \"comment\": \"Depth\", \"display_name\": \"Depth\"}),\n",
    "    StructField(\"height\", DoubleType(), True, metadata={\"description\": \"Height of the product\", \"comment\": \"Height\", \"display_name\": \"Height\"}),\n",
    "    StructField(\"dimension_unit\", StringType(), True, metadata={\"description\": \"Unit for product dimensions\", \"comment\": \"Dimension unit\", \"display_name\": \"Dimension Unit\"}),\n",
    "    StructField(\"product_pic_url\", StringType(), True, metadata={\"description\": \"URL to the product image\", \"comment\": \"Product picture URL\", \"display_name\": \"Product Picture URL\"})\n",
    "])\n",
    "\n",
    "gh_full_file_name = f\"{GITHUB_PATH}/{file_name}\"\n",
    "display(f\"Downloading {gh_full_file_name}\")\n",
    "\n",
    "sc.addFile(gh_full_file_name)\n",
    "gh_file_name  = 'file://' +SparkFiles.get(file_name)\n",
    "\n",
    "products_df = spark.read.csv(path=gh_file_name,header=True,schema=schema)\n",
    "print(f\"Downloaded data {gh_file_name}\")\n",
    "\n",
    "products_df = products_df.withColumn(\"created_date\", to_date(col(\"created_date\"), \"yyyyMMdd\"))\n",
    "products_df = products_df.withColumn(\"modified_date\", to_date(col(\"modified_date\"), \"yyyyMMdd\"))\n",
    "\n",
    "products_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"products\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02927df5-5b02-49f1-9680-6aaa989bd3e1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Load Product Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d22f25-17a9-4bb4-a54c-1d5e4ee9964c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-07T19:48:34.6829882Z",
       "execution_start_time": "2026-02-07T19:48:24.6142128Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "1756d362-1797-46b8-bce0-c3e10b3ff84c",
       "queued_time": "2026-02-07T19:48:24.6131428Z",
       "session_id": "cbc235ee-e949-4a4f-bc4e-8f3159d74f2d",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 7,
       "statement_ids": [
        7
       ]
      },
      "text/plain": [
       "StatementMeta(, cbc235ee-e949-4a4f-bc4e-8f3159d74f2d, 7, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Downloading https://raw.githubusercontent.com/martinabrle/demo-data/main/Sample_Bike_Sales/ProductTexts.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded data file:///mnt/var/hadoop/tmp/nm-local-dir/usercache/trusted-service-user/appcache/application_1770492411878_0001/spark-400262e1-c607-4b54-bca3-63b28d3d28fd/userFiles-fcb2385f-723c-40b8-9274-532143b2746f/ProductTexts.csv\n"
     ]
    }
   ],
   "source": [
    "file_name=\"ProductTexts.csv\"\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"product_id\", StringType(), False, metadata={\"description\": \"Unique identifier of the product\", \"comment\": \"Product ID\", \"display_name\": \"Product ID\"}),\n",
    "    StructField(\"language\", StringType(), False, metadata={\"description\": \"Language code for the product description\", \"comment\": \"Language\", \"display_name\": \"Language\"}),\n",
    "    StructField(\"short_description\", StringType(), True, metadata={\"description\": \"Product\", \"comment\": \"Short description\", \"display_name\": \"Short Description\"}),\n",
    "    StructField(\"medium_description\", StringType(), True, metadata={\"description\": \"Medium-length description of the product\", \"comment\": \"Medium description\", \"display_name\": \"Medium Description\"}),\n",
    "    StructField(\"long_description\", StringType(), True, metadata={\"description\": \"Long, detailed description of the product\", \"comment\": \"Long description\", \"display_name\": \"Long Description\"})\n",
    "])\n",
    "\n",
    "gh_full_file_name = f\"{GITHUB_PATH}/{file_name}\"\n",
    "display(f\"Downloading {gh_full_file_name}\")\n",
    "\n",
    "sc.addFile(gh_full_file_name)\n",
    "gh_file_name  = 'file://' +SparkFiles.get(file_name)\n",
    "\n",
    "product_texts_df = spark.read.csv(path=gh_file_name,header=True,schema=schema)\n",
    "print(f\"Downloaded data {gh_file_name}\")\n",
    "\n",
    "product_texts_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"product_texts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93c1b45-1f3d-4c8d-b508-6b984997c8ec",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Load Sales Orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e3fba2-ef32-471f-9987-3e90397be8e4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-08T09:57:40.9135174Z",
       "execution_start_time": "2026-02-08T09:57:20.5343189Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "8ccf672c-047f-4e0e-a7e1-6e75dc93506f",
       "queued_time": "2026-02-08T09:57:20.5330712Z",
       "session_id": "0958bb78-4d2e-4bb0-8a5e-e96cb671914c",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 7,
       "statement_ids": [
        7
       ]
      },
      "text/plain": [
       "StatementMeta(, 0958bb78-4d2e-4bb0-8a5e-e96cb671914c, 7, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Downloading https://raw.githubusercontent.com/martinabrle/demo-data/main/Sample_Bike_Sales/SalesOrders.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded data file:///mnt/var/hadoop/tmp/nm-secondary-local-dir/usercache/trusted-service-user/appcache/application_1770543756420_0001/spark-ba0537c7-9332-4ab2-a377-5e54be62443f/userFiles-d84a831e-0708-41c7-b574-f77b3fc6db24/SalesOrders.csv\n"
     ]
    }
   ],
   "source": [
    "file_name=\"SalesOrders.csv\"\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"sales_order_id\", IntegerType(), False, metadata={\"description\": \"Unique identifier of the sales order\", \"comment\": \"Sales order ID\", \"display_name\": \"Sales Order ID\", \"primary_key\": True}),\n",
    "    StructField(\"created_by\", IntegerType(), True, metadata={\"description\": \"Identifier of the user who created the sales order\", \"comment\": \"Created by\", \"display_name\": \"Created By\"}),\n",
    "    StructField(\"created_date\", StringType(), True, metadata={\"description\": \"Date when the sales order was created\", \"comment\": \"Created date\", \"display_name\": \"Created Date\"}),\n",
    "    StructField(\"modified_by\", IntegerType(), True, metadata={\"description\": \"Identifier of the user who last modified the sales order\", \"comment\": \"Modified by\", \"display_name\": \"Modified By\"}),\n",
    "    StructField(\"modified_date\", StringType(), True, metadata={\"description\": \"Date when the sales order was last modified\", \"comment\": \"Modified date\", \"display_name\": \"Modified Date\"}),\n",
    "    StructField(\"fisc_variant\", StringType(), True, metadata={\"description\": \"Fiscal variant associated with the sales order\", \"comment\": \"Fiscal variant\", \"display_name\": \"Fiscal Variant\"}),\n",
    "    StructField(\"fiscal_year_period\", StringType(), True, metadata={\"description\": \"Fiscal year and period for the sales order\", \"comment\": \"Fiscal year period\", \"display_name\": \"Fiscal Year/Period\"}),\n",
    "    StructField(\"note_id\", IntegerType(), True, metadata={\"description\": \"Identifier of associated note\", \"comment\": \"Note ID\", \"display_name\": \"Note ID\"}),\n",
    "    StructField(\"customer_id\", IntegerType(), False, metadata={\"description\": \"Identifier of the customer for this sales order\", \"comment\": \"Customer ID\", \"display_name\": \"Customer ID\"}),\n",
    "    StructField(\"sales_org\", StringType(), True, metadata={\"description\": \"Sales organization responsible for the order\", \"comment\": \"Sales organization\", \"display_name\": \"Sales Organization\"}),\n",
    "    StructField(\"currency\", StringType(), True, metadata={\"description\": \"Currency of the sales order amounts\", \"comment\": \"Currency\", \"display_name\": \"Currency\"}),\n",
    "    StructField(\"gross_amount\", DecimalType(), True, metadata={\"description\": \"Gross amount of the sales order\", \"comment\": \"Gross amount\", \"display_name\": \"Gross Amount\"}),\n",
    "    StructField(\"net_amount\", DecimalType(), True, metadata={\"description\": \"Net amount of the sales order\", \"comment\": \"Net amount\", \"display_name\": \"Net Amount\"}),\n",
    "    StructField(\"tax_amount\", DecimalType(), True, metadata={\"description\": \"Tax amount of the sales order\", \"comment\": \"Tax amount\", \"display_name\": \"Tax Amount\"}),\n",
    "    StructField(\"lifecycle_status\", StringType(), True, metadata={\"description\": \"Lifecycle status of the sales order (In Process, Completed, Cancelled, Unknown)\", \"comment\": \"Lifecycle status\", \"display_name\": \"Lifecycle Status\"}),\n",
    "    StructField(\"billing_status\", StringType(), True, metadata={\"description\": \"Billing status of the sales order (Not Billed, Partially Billed, Completely Billed, Cancelled, Unknown)\", \"comment\": \"Billing status\", \"display_name\": \"Billing Status\"}),\n",
    "    StructField(\"delivery_status\", StringType(), True, metadata={\"description\": \"Delivery status of the sales order (Not Delivered, Partially Delivered, Completely Delivered, Cancelled, Unknown)\", \"comment\": \"Delivery status\", \"display_name\": \"Delivery Status\"})\n",
    "])\n",
    "\n",
    "gh_full_file_name = f\"{GITHUB_PATH}/{file_name}\"\n",
    "display(f\"Downloading {gh_full_file_name}\")\n",
    "\n",
    "sc.addFile(gh_full_file_name)\n",
    "gh_file_name  = 'file://' +SparkFiles.get(file_name)\n",
    "\n",
    "sales_orders_df = spark.read.csv(path=gh_file_name,header=True,schema=schema)\n",
    "print(f\"Downloaded data {gh_file_name}\")\n",
    "\n",
    "sales_orders_df = sales_orders_df.withColumn(\"created_date\", to_date(col(\"created_date\"), \"yyyyMMdd\"))\n",
    "sales_orders_df = sales_orders_df.withColumn(\"modified_date\", to_date(col(\"modified_date\"), \"yyyyMMdd\"))\n",
    "\n",
    "sales_orders_df = sales_orders_df.withColumn(\n",
    "    \"lifecycle_status\",\n",
    "    F.when(F.col(\"lifecycle_status\") == \"I\", \"In Process\")\n",
    "     .when(F.col(\"lifecycle_status\") == \"C\", \"Completed\")\n",
    "     .when(F.col(\"lifecycle_status\") == \"X\", \"Cancelled\")\n",
    "     .otherwise(\"Unknown\")\n",
    ")\n",
    "\n",
    "sales_orders_df = sales_orders_df.withColumn(\n",
    "    \"billing_status\",\n",
    "    F.when(F.col(\"billing_status\") == \"N\", \"Not Billed\")\n",
    "     .when(F.col(\"billing_status\") == \"P\", \"Partially Billed\")\n",
    "     .when(F.col(\"billing_status\") == \"C\", \"Completely Billed\")\n",
    "     .when(F.col(\"billing_status\") == \"X\", \"Cancelled\")\n",
    "     .otherwise(\"Unknown\")\n",
    ")\n",
    "\n",
    "sales_orders_df = sales_orders_df.withColumn(\n",
    "    \"delivery_status\",\n",
    "    F.when(F.col(\"delivery_status\") == \"N\", \"Not Delivered\")\n",
    "     .when(F.col(\"delivery_status\") == \"P\", \"Partially Delivered\")\n",
    "     .when(F.col(\"delivery_status\") == \"C\", \"Completely Delivered\")\n",
    "     .when(F.col(\"delivery_status\") == \"X\", \"Cancelled\")\n",
    "     .otherwise(\"Unknown\")\n",
    ")\n",
    "\n",
    "sales_orders_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"sales_orders\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc7206d-0ed3-4806-b00c-7d278305c973",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Load Sales Order Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37ae87f2-1506-482b-b2d3-156dc09ddb77",
   "metadata": {
    "advisor": {
     "adviceMetadata": "{\"artifactId\":\"621b7fe4-1488-49ca-917b-6a36e2774b21\",\"activityId\":\"cbc235ee-e949-4a4f-bc4e-8f3159d74f2d\",\"applicationId\":\"application_1770492411878_0001\",\"jobGroupId\":\"5\",\"advices\":{\"info\":1}}"
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-07T19:47:56.5087363Z",
       "execution_start_time": "2026-02-07T19:47:18.8977866Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "13890287-f8ec-460d-b237-396850b97b14",
       "queued_time": "2026-02-07T19:47:18.8965676Z",
       "session_id": "cbc235ee-e949-4a4f-bc4e-8f3159d74f2d",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 5,
       "statement_ids": [
        5
       ]
      },
      "text/plain": [
       "StatementMeta(, cbc235ee-e949-4a4f-bc4e-8f3159d74f2d, 5, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Downloading https://raw.githubusercontent.com/martinabrle/demo-data/main/Sample_Bike_Sales/SalesOrderItems.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded data file:///mnt/var/hadoop/tmp/nm-local-dir/usercache/trusted-service-user/appcache/application_1770492411878_0001/spark-400262e1-c607-4b54-bca3-63b28d3d28fd/userFiles-fcb2385f-723c-40b8-9274-532143b2746f/SalesOrderItems.csv\n"
     ]
    }
   ],
   "source": [
    "file_name=\"SalesOrderItems.csv\"\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"sales_order_id\", IntegerType(), False, metadata={\"description\": \"Identifier of the sales order\", \"comment\": \"Sales order ID\", \"display_name\": \"Sales Order ID\"}),\n",
    "    StructField(\"sales_order_item_id\", IntegerType(), False, metadata={\"description\": \"Identifier of the sales order item\", \"comment\": \"Sales order item ID\", \"display_name\": \"Sales Order Item ID\"}),\n",
    "    StructField(\"product_id\", StringType(), False, metadata={\"description\": \"Identifier of the product for this order item\", \"comment\": \"Product ID\", \"display_name\": \"Product ID\"}),\n",
    "    StructField(\"note_id\", IntegerType(), True, metadata={\"description\": \"Identifier of associated note for this item\", \"comment\": \"Note ID\", \"display_name\": \"Note ID\"}),\n",
    "    StructField(\"currency\", StringType(), True, metadata={\"description\": \"Currency of the amounts for this order item\", \"comment\": \"Currency\", \"display_name\": \"Currency\"}),\n",
    "    StructField(\"gross_amount\", DecimalType(), True, metadata={\"description\": \"Gross amount for this order item\", \"comment\": \"Gross amount\", \"display_name\": \"Gross Amount\"}),\n",
    "    StructField(\"net_amount\", DecimalType(), True, metadata={\"description\": \"Net amount for this order item\", \"comment\": \"Net amount\", \"display_name\": \"Net Amount\"}),\n",
    "    StructField(\"tax_amount\", DecimalType(), True, metadata={\"description\": \"Tax amount for this order item\", \"comment\": \"Tax amount\", \"display_name\": \"Tax Amount\"}),\n",
    "    StructField(\"item_atp_status\", StringType(), True, metadata={\"description\": \"Available-to-promise status of the order item (Confirmed, Partially Confirmed, Not Confirmed, Cancelled / Not Relevant, Unknown)\", \"comment\": \"Item ATP status\", \"display_name\": \"Item ATP Status\"}),\n",
    "    StructField(\"op_item_pos\", StringType(), True, metadata={\"description\": \"Operational item position identifier\", \"comment\": \"Operational item position\", \"display_name\": \"OP Item Pos\"}),\n",
    "    StructField(\"quantity\", DecimalType(), True, metadata={\"description\": \"Quantity ordered for this item\", \"comment\": \"Quantity\", \"display_name\": \"Quantity\"}),\n",
    "    StructField(\"quantity_unit\", DecimalType(), True, metadata={\"description\": \"Unit of measure for the ordered quantity\", \"comment\": \"Quantity unit\", \"display_name\": \"Quantity Unit\"}),\n",
    "    StructField(\"delivery_date\", DateType(), True, metadata={\"description\": \"Planned delivery date for the order item\", \"comment\": \"Delivery date\", \"display_name\": \"Delivery Date\"})\n",
    "])\n",
    "\n",
    "gh_full_file_name = f\"{GITHUB_PATH}/{file_name}\"\n",
    "display(f\"Downloading {gh_full_file_name}\")\n",
    "\n",
    "sc.addFile(gh_full_file_name)\n",
    "gh_file_name  = 'file://' +SparkFiles.get(file_name)\n",
    "\n",
    "sales_order_items_df = spark.read.csv(path=gh_file_name,header=True,schema=schema)\n",
    "print(f\"Downloaded data {gh_file_name}\")\n",
    "\n",
    "sales_order_items_df = sales_order_items_df.withColumn(\"delivery_date\", to_date(col(\"delivery_date\"), \"yyyyMMdd\"))\n",
    "\n",
    "sales_order_items_df = sales_order_items_df.withColumn(\n",
    "    \"item_atp_status\",\n",
    "    F.when(F.col(\"item_atp_status\") == \"C\", \"Confirmed\")\n",
    "     .when(F.col(\"item_atp_status\") == \"P\", \"Partially Confirmed\")\n",
    "     .when(F.col(\"item_atp_status\") == \"N\", \"Not Confirmed\")\n",
    "     .when(F.col(\"item_atp_status\") == \"X\", \"Cancelled / Not Relevant\")\n",
    "     .otherwise(\"Unknown\")\n",
    ")\n",
    "sales_order_items_df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(\"sales_order_items\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bbf7b3-258f-4197-b766-3dca281f5773",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Display table info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eed597-175d-4aa3-b8ef-a534fb1a18ce",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-07T16:01:11.2875433Z",
       "execution_start_time": null,
       "livy_statement_state": null,
       "normalized_state": "cancelled",
       "parent_msg_id": "6fef98e6-d013-4b98-96ed-71a56da195a3",
       "queued_time": "2026-02-07T15:58:23.9679288Z",
       "session_id": "7e17825f-b6fa-45fb-b49f-f2aecc437fb6",
       "session_start_time": null,
       "spark_pool": null,
       "state": "cancelled",
       "statement_id": -1,
       "statement_ids": null
      },
      "text/plain": [
       "StatementMeta(, 7e17825f-b6fa-45fb-b49f-f2aecc437fb6, -1, Cancelled, , Cancelled)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_table_metadata(table_name):\n",
    "    df=spark.table(table_name)\n",
    "    \"\"\"\n",
    "    Prints schema details and metadata for a Spark DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df: Spark DataFrame\n",
    "    \"\"\"\n",
    "    print(\" \")\n",
    "    print(\" \")\n",
    "    print(f\"Table: {table_name}:\")\n",
    "    for field in df.schema.fields:\n",
    "        print(f\"Column: {field.name}\")\n",
    "        print(f\"  Type: {field.dataType}\")\n",
    "        print(f\"  Nullable: {field.nullable}\")\n",
    "        # Print metadata nicely if it exists\n",
    "        if field.metadata:\n",
    "            for key, value in field.metadata.items():\n",
    "                print(f\"  {key}: {value}\")\n",
    "        else:\n",
    "            print(\"  Metadata: None\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15b940f-8976-45fa-9b85-6db8b6a5f2b9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "#Check Metadata has been successfully written\n",
    "show_table_metadata(\"sales_order_items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6016ae-7cca-4981-a6f4-969fa667266a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Check data consistency - Vendors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b787143e-47c5-4441-83ad-6029ba3bda35",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-08T10:08:38.8011235Z",
       "execution_start_time": "2026-02-08T10:08:15.4537007Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "d792d1e8-52af-47ea-a7ef-94784a337f56",
       "queued_time": "2026-02-08T10:08:15.4527152Z",
       "session_id": "0958bb78-4d2e-4bb0-8a5e-e96cb671914c",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 9,
       "statement_ids": [
        9
       ]
      },
      "text/plain": [
       "StatementMeta(, 0958bb78-4d2e-4bb0-8a5e-e96cb671914c, 9, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.synapse.widget-view+json": {
       "widget_id": "8393ed1e-0689-40a2-b0e9-c0af8bab768e",
       "widget_type": "Synapse.DataFrame"
      },
      "text/plain": [
       "SynapseWidget(Synapse.DataFrame, 8393ed1e-0689-40a2-b0e9-c0af8bab768e)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.synapse.widget-view+json": {
       "widget_id": "2aca9896-f7f5-4827-98df-ebfd6e0e1156",
       "widget_type": "Synapse.DataFrame"
      },
      "text/plain": [
       "SynapseWidget(Synapse.DataFrame, 2aca9896-f7f5-4827-98df-ebfd6e0e1156)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vendors_df = spark.table(\"vendors\")\n",
    "vendor_addresses_df = spark.table(\"vendor_addresses\")\n",
    "\n",
    "vendors_with_missing_addresses_df = (\n",
    "    vendors_df\n",
    "        .join(\n",
    "            vendor_addresses_df,\n",
    "            vendors_df.address_id == vendor_addresses_df.address_id,\n",
    "            how=\"left_anti\"\n",
    "        )\n",
    ")\n",
    "display(vendors_with_missing_addresses_df)\n",
    "\n",
    "addresses_with_missing_vendors_df = (\n",
    "    vendor_addresses_df\n",
    "        .join(\n",
    "            vendors_df,\n",
    "            vendor_addresses_df.address_id == vendors_df.address_id,\n",
    "            how=\"left_anti\"\n",
    "        )\n",
    ")\n",
    "\n",
    "display(addresses_with_missing_vendors_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f467b6-4f22-4c58-847f-d8b2de6accce",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Check data consistency - Customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ec21c61-c02e-4d67-a2f5-567379da21a0",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-08T10:08:42.2614133Z",
       "execution_start_time": "2026-02-08T10:08:38.8033815Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "2adbfff5-3fb0-4642-96a9-c4c9a6009d33",
       "queued_time": "2026-02-08T10:08:27.0466593Z",
       "session_id": "0958bb78-4d2e-4bb0-8a5e-e96cb671914c",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 10,
       "statement_ids": [
        10
       ]
      },
      "text/plain": [
       "StatementMeta(, 0958bb78-4d2e-4bb0-8a5e-e96cb671914c, 10, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.synapse.widget-view+json": {
       "widget_id": "cfc1b496-1dca-48ee-be3f-f15d7d4d3ae9",
       "widget_type": "Synapse.DataFrame"
      },
      "text/plain": [
       "SynapseWidget(Synapse.DataFrame, cfc1b496-1dca-48ee-be3f-f15d7d4d3ae9)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.synapse.widget-view+json": {
       "widget_id": "29da78fc-7378-411a-b6dc-61a981a69e0c",
       "widget_type": "Synapse.DataFrame"
      },
      "text/plain": [
       "SynapseWidget(Synapse.DataFrame, 29da78fc-7378-411a-b6dc-61a981a69e0c)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "customers_df = spark.table(\"vendors\")\n",
    "customer_addresses_df = spark.table(\"vendor_addresses\")\n",
    "\n",
    "customers_with_missing_addresses_df = (\n",
    "    customers_df\n",
    "        .join(\n",
    "            customer_addresses_df,\n",
    "            customers_df.address_id == customer_addresses_df.address_id,\n",
    "            how=\"left_anti\"\n",
    "        )\n",
    ")\n",
    "display(customers_with_missing_addresses_df)\n",
    "\n",
    "addresses_with_missing_customers_df = (\n",
    "    customer_addresses_df\n",
    "        .join(\n",
    "            customers_df,\n",
    "            customer_addresses_df.address_id == customers_df.address_id,\n",
    "            how=\"left_anti\"\n",
    "        )\n",
    ")\n",
    "\n",
    "display(addresses_with_missing_customers_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cae148-d3cb-4236-b62b-dfda3d9223fd",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Check data consistency - Employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d601936-e611-4302-90ea-be2a505a755b",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-08T10:08:44.748531Z",
       "execution_start_time": "2026-02-08T10:08:42.2635278Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "43b6ec16-76b6-45fb-bb2a-b1b9be0b6908",
       "queued_time": "2026-02-08T10:08:35.8658859Z",
       "session_id": "0958bb78-4d2e-4bb0-8a5e-e96cb671914c",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 11,
       "statement_ids": [
        11
       ]
      },
      "text/plain": [
       "StatementMeta(, 0958bb78-4d2e-4bb0-8a5e-e96cb671914c, 11, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.synapse.widget-view+json": {
       "widget_id": "7e28ccc5-78fe-49ba-b607-97e0bbd62ae3",
       "widget_type": "Synapse.DataFrame"
      },
      "text/plain": [
       "SynapseWidget(Synapse.DataFrame, 7e28ccc5-78fe-49ba-b607-97e0bbd62ae3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.synapse.widget-view+json": {
       "widget_id": "3ec29633-6862-4e4a-9617-a0e60f4b784c",
       "widget_type": "Synapse.DataFrame"
      },
      "text/plain": [
       "SynapseWidget(Synapse.DataFrame, 3ec29633-6862-4e4a-9617-a0e60f4b784c)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "employees_df = spark.table(\"vendors\")\n",
    "employee_addresses_df = spark.table(\"vendor_addresses\")\n",
    "\n",
    "employees_with_missing_addresses_df = (\n",
    "    employees_df\n",
    "        .join(\n",
    "            employee_addresses_df,\n",
    "            employees_df.address_id == employee_addresses_df.address_id,\n",
    "            how=\"left_anti\"\n",
    "        )\n",
    ")\n",
    "display(employees_with_missing_addresses_df)\n",
    "\n",
    "addresses_with_missing_employees_df = (\n",
    "    employee_addresses_df\n",
    "        .join(\n",
    "            employees_df,\n",
    "            employee_addresses_df.address_id == employees_df.address_id,\n",
    "            how=\"left_anti\"\n",
    "        )\n",
    ")\n",
    "\n",
    "display(addresses_with_missing_employees_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de64121-7abc-4b12-a27d-58d618180118",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**Retrieve access token**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c501bf-414f-4911-b12f-0b1a593a413d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "scope = \"https://analysis.windows.net/powerbi/api/.default\"\n",
    "\n",
    "url = f\"https://login.microsoftonline.com/{TENANT_ID}/oauth2/v2.0/token\"\n",
    "\n",
    "data = {\n",
    "    \"grant_type\": \"client_credentials\",\n",
    "    \"client_id\": CLIENT_ID,\n",
    "    \"client_secret\": CLIENT_SECRET,\n",
    "    \"scope\": scope\n",
    "}\n",
    "\n",
    "response = requests.post(url, data=data)\n",
    "access_token = response.json().get(\"access_token\")\n",
    "print(\"Bearer token:\", access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceaae2c-ca29-4264-837d-4cca80cdf77e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# --- Configuration ---\n",
    "workspace_name = \"demo-bike-sales\"\n",
    "xmla_url = f\"https://{workspace_name}.asazure.windows.net/xmla?role=Admin\"\n",
    "\n",
    "# Replace this with a valid bearer token obtained in the notebook\n",
    "\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"text/xml\",\n",
    "    \"Authorization\": f\"Bearer {access_token}\"\n",
    "}\n",
    "\n",
    "# Minimal XMLA Discover request to list catalogs\n",
    "xmla_request = \"\"\"<?xml version=\"1.0\"?>\n",
    "<Envelope xmlns=\"http://schemas.xmlsoap.org/soap/envelope/\">\n",
    "  <Body>\n",
    "    <Discover xmlns=\"urn:schemas-microsoft-com:xml-analysis\">\n",
    "      <RequestType>DBSCHEMA_CATALOGS</RequestType>\n",
    "      <Restrictions/>\n",
    "      <Properties/>\n",
    "    </Discover>\n",
    "  </Body>\n",
    "</Envelope>\n",
    "\"\"\"\n",
    "\n",
    "# Send request\n",
    "response = requests.post(xmla_url, headers=headers, data=xmla_request)\n",
    "\n",
    "# Print status and partial content\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Response (first 500 chars):\")\n",
    "print(response.text[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2589a985-59d6-47c7-8d8c-88c15e4020e6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-06T13:43:55.3490355Z",
       "execution_start_time": "2026-02-06T13:43:55.0807057Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "b3cc6700-6e6d-410e-bd93-2b9abcf9cff8",
       "queued_time": "2026-02-06T13:43:55.0795007Z",
       "session_id": "70648f3f-687c-463f-a292-3bf7ebb83fd9",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 15,
       "statement_ids": [
        15
       ]
      },
      "text/plain": [
       "StatementMeta(, 70648f3f-687c-463f-a292-3bf7ebb83fd9, 15, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'semantic_model' from 'fabric' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfabric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m semantic_model\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msempy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtables\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Table\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_lakehouse_metadata\u001b[39m(table_name: \u001b[38;5;28mstr\u001b[39m, token: \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'semantic_model' from 'fabric' (unknown location)"
     ]
    }
   ],
   "source": [
    "from fabric import semantic_model\n",
    "from sempy.tables import Table\n",
    "\n",
    "def update_lakehouse_metadata(table_name: str, token: str):\n",
    "    \"\"\"\n",
    "    Connects to a Fabric workspace semantic model, loops over columns\n",
    "    of the given Lakehouse table, retrieves metadata, and updates \n",
    "    description, comment, and display_name.\n",
    "\n",
    "    Parameters:\n",
    "        table_name (str): Lakehouse table name (e.g., 'product_texts')\n",
    "        token (str): Bearer token with XMLA/Admin access\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load Spark table\n",
    "    df = spark.table(f\"{database_name}.{table_name}\")\n",
    "\n",
    "    # Connect to semantic model\n",
    "    model = Model(workspace_name=WORKSPACE_NAME,token=token)\n",
    "    \n",
    "    # Get the table object\n",
    "    table = model.get_table(table_name)\n",
    "    \n",
    "    print(f\"Updating metadata for table: {table_name}\")\n",
    "    \n",
    "    for field in df.schema.fields:\n",
    "        col_name = field.name\n",
    "        col_meta = field.metadata if field.metadata else {}\n",
    "        \n",
    "        # Retrieve metadata values\n",
    "        display_name = col_meta.get(\"display_name\", col_name.replace(\"_\", \" \").title())\n",
    "        description  = col_meta.get(\"description\", \"\")\n",
    "        comment      = col_meta.get(\"comment\", \"\")\n",
    "        \n",
    "        # Get the column in the semantic model\n",
    "        col = table.get_column(col_name)\n",
    "        \n",
    "        # Print current and new metadata\n",
    "        print(f\"Column: {col_name}\")\n",
    "        print(f\"  Current display_name: {col.display_name}\")\n",
    "        print(f\"  Current description: {col.description}\")\n",
    "        print(f\"  Current comment: {col.comment}\")\n",
    "        print(f\"  New display_name: {display_name}\")\n",
    "        print(f\"  New description: {description}\")\n",
    "        print(f\"  New comment: {comment}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Update semantic model metadata\n",
    "        col.display_name = display_name\n",
    "        col.description = description\n",
    "        col.comment = comment\n",
    "        col.save()\n",
    "    \n",
    "    update_lakehouse_metadata(\"products\", bearer_token)\n",
    "    print(f\"Metadata update completed for table: {table_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcc1433-0aaa-4dc2-b426-09da1fd0bac2",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "update_lakehouse_metadata(\"products\", bearer_token, workspace_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441f1715-e8a4-48c1-bf5f-8aba35ad1485",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Update semantic model's metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1087f55a-abed-4c93-a6c0-0bac3f1d87d6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-06T13:24:49.9772445Z",
       "execution_start_time": "2026-02-06T13:24:49.500199Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "57815c40-85d3-4a4c-bdeb-f316727de939",
       "queued_time": "2026-02-06T13:24:49.4988661Z",
       "session_id": "70648f3f-687c-463f-a292-3bf7ebb83fd9",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 9,
       "statement_ids": [
        9
       ]
      },
      "text/plain": [
       "StatementMeta(, 70648f3f-687c-463f-a292-3bf7ebb83fd9, 9, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def update_semantic_model_from_spark(df, xmla_url, bearer_token, database_name, table_name):\n",
    "    \"\"\"\n",
    "    Updates a Fabric / Power BI semantic model table's column properties\n",
    "    using Spark DataFrame metadata (display_name, description, nullable, comment).\n",
    "    \n",
    "    Args:\n",
    "        df: Spark DataFrame with metadata in schema\n",
    "        xmla_url: XMLA endpoint URL (Admin role)\n",
    "        bearer_token: Azure AD access token with workspace permissions\n",
    "        database_name: Semantic model (dataset) name\n",
    "        table_name: Table name in the semantic model\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"text/xml\",\n",
    "        \"Authorization\": f\"Bearer {bearer_token}\"\n",
    "    }\n",
    "\n",
    "    # Build XMLA payload for multiple columns in one request\n",
    "    columns_xml = \"\"\n",
    "    for field in df.schema.fields:\n",
    "        column_name = field.name\n",
    "        display_name = field.metadata.get(\"display_name\", column_name)\n",
    "        description = field.metadata.get(\"description\", \"\")\n",
    "        nullable = str(field.nullable).lower()  # 'true' or 'false'\n",
    "        comment = field.metadata.get(\"comment\", \"\")\n",
    "\n",
    "        # Each column alteration XML\n",
    "        columns_xml += f\"\"\"\n",
    "        <Column>\n",
    "            <ColumnID>{column_name}</ColumnID>\n",
    "            <Name>{display_name}</Name>\n",
    "            <Description>{description}</Description>\n",
    "            <IsNullable>{nullable}</IsNullable>\n",
    "            <Annotations>\n",
    "                <Annotation>\n",
    "                    <Name>comment</Name>\n",
    "                    <Value>{comment}</Value>\n",
    "                </Annotation>\n",
    "            </Annotations>\n",
    "        </Column>\n",
    "        \"\"\"\n",
    "\n",
    "    # Full XMLA request\n",
    "    xmla_payload = f\"\"\"<?xml version=\"1.0\"?>\n",
    "<Envelope xmlns=\"http://schemas.xmlsoap.org/soap/envelope/\">\n",
    "  <Body>\n",
    "    <Execute xmlns=\"urn:schemas-microsoft-com:xml-analysis\">\n",
    "      <Command>\n",
    "        <Alter xmlns=\"http://schemas.microsoft.com/analysisservices/2003/engine\">\n",
    "          <Object>\n",
    "            <DatabaseID>{database_name}</DatabaseID>\n",
    "            <CubeID>{table_name}</CubeID>\n",
    "          </Object>\n",
    "          <AlterCommand>\n",
    "            <Table>\n",
    "                {columns_xml}\n",
    "            </Table>\n",
    "          </AlterCommand>\n",
    "        </Alter>\n",
    "      </Command>\n",
    "      <Properties>\n",
    "        <PropertyList>\n",
    "          <LocaleIdentifier>1033</LocaleIdentifier>\n",
    "        </PropertyList>\n",
    "      </Properties>\n",
    "    </Execute>\n",
    "  </Body>\n",
    "</Envelope>\n",
    "\"\"\"\n",
    "\n",
    "    response = requests.post(xmla_url, headers=headers, data=xmla_payload)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to update table {table_name}: {response.text}\")\n",
    "    else:\n",
    "        print(f\"Updated table {table_name} columns successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "407b4d79-80f6-4c17-9aae-3e578f7ba1f5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-06T13:23:17.0783027Z",
       "execution_start_time": "2026-02-06T13:23:16.0245096Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "d65a78fd-5521-4963-be48-8a5a79d53901",
       "queued_time": "2026-02-06T13:23:16.0233383Z",
       "session_id": "70648f3f-687c-463f-a292-3bf7ebb83fd9",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 5,
       "statement_ids": [
        5
       ]
      },
      "text/plain": [
       "StatementMeta(, 70648f3f-687c-463f-a292-3bf7ebb83fd9, 5, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bearer token: eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIsIng1dCI6IlBjWDk4R1g0MjBUMVg2c0JEa3poUW1xZ3dNVSIsImtpZCI6IlBjWDk4R1g0MjBUMVg2c0JEa3poUW1xZ3dNVSJ9.eyJhdWQiOiJodHRwczovL2FuYWx5c2lzLndpbmRvd3MubmV0L3Bvd2VyYmkvYXBpIiwiaXNzIjoiaHR0cHM6Ly9zdHMud2luZG93cy5uZXQvM2JiMWQzNDgtMTE5NS00MjY3LTllMDItYWMzNDI1Nzc4MGZjLyIsImlhdCI6MTc3MDM4Mzg5NiwibmJmIjoxNzcwMzgzODk2LCJleHAiOjE3NzAzODc3OTYsImFpbyI6ImsyWmdZTmoyYUx2TnRlTVhWTGtjZGk2ODRlWnNBZ0E9IiwiYXBwaWQiOiIyYmY1ZjkyZS0wYTdjLTQ1NGEtOGNjOC0zMWY3MzViZGY3NmMiLCJhcHBpZGFjciI6IjEiLCJpZHAiOiJodHRwczovL3N0cy53aW5kb3dzLm5ldC8zYmIxZDM0OC0xMTk1LTQyNjctOWUwMi1hYzM0MjU3NzgwZmMvIiwiaWR0eXAiOiJhcHAiLCJvaWQiOiJlNWU1OTc4NS1hMTFhLTQ5NjktYTZjZi1lY2IxMTM2MGNjZTAiLCJyaCI6IjEuQVVzQVNOT3hPNVVSWjBLZUFxdzBKWGVBX0FrQUFBQUFBQUFBd0FBQUFBQUFBQUFBQUFCTEFBLiIsInN1YiI6ImU1ZTU5Nzg1LWExMWEtNDk2OS1hNmNmLWVjYjExMzYwY2NlMCIsInRpZCI6IjNiYjFkMzQ4LTExOTUtNDI2Ny05ZTAyLWFjMzQyNTc3ODBmYyIsInV0aSI6IjBTaEhYN0tGcGstQUp1UklaWklkQUEiLCJ2ZXIiOiIxLjAiLCJ4bXNfYWN0X2ZjdCI6IjMgOSIsInhtc19mdGQiOiJsTjZBMGNPVWg4bThJdWtVbFpSY3dUaUhDTXgzVW40SGN0OXN0RjI5ZGRVQlpYVnliM0JsYm05eWRHZ3RaSE50Y3ciLCJ4bXNfaWRyZWwiOiIyMCA3IiwieG1zX3JkIjoiMC40MkxsWUJKaTlCWVM0V0FYRXBqTHhUd3pkNDZEMzdLRXlpbFRUUzlkQklweUNnbmtYUlNjdFhqeFFmLWwzajZUVjdDN3FRRkZPWVFFT0JrZzRBQ1VCb3B5Q3drYzQtelp1Q2lfVi1CdF9ObkwybzE3amdNQSIsInhtc19zdWJfZmN0IjoiMyA5In0.EUxL09l_lsfjRSVkeiIe6-J_KBat79_F5zygCJGqSKUPyixv-1NwHLPlSWo4ZdNczudcDeZHxNHKZSNSbWWxWWXEPRHBBywlLFuPumW27oqFnn7m-06blctZ-T_2lOSLoBZApSns1VuT9bLXBCke55RXPSDWVXqMm-0Wjb-dz95RP6Uw1VbHavFUvnBmLgVNDxBr0LDSmInIA4F-qaa7e_D0SXJ6Q981vF_fLZiot1ZYbLcpyd3LSTx31y5QPFw_wy52KcIf1eWz4_aB0YYKf2-kcu-dRDj8Mvf6O1fRsgDcBFUtbGp43ITddlZVnfZ0jBKnYbAQ9-bX7W30DRN_lA\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "scope = \"https://analysis.windows.net/powerbi/api/.default\"\n",
    "\n",
    "url = f\"https://login.microsoftonline.com/{TENANT_ID}/oauth2/v2.0/token\"\n",
    "\n",
    "data = {\n",
    "    \"grant_type\": \"client_credentials\",\n",
    "    \"client_id\": CLIENT_ID,\n",
    "    \"client_secret\": CLIENT_SECRET,\n",
    "    \"scope\": scope\n",
    "}\n",
    "\n",
    "response = requests.post(url, data=data)\n",
    "access_token = response.json().get(\"access_token\")\n",
    "print(\"Bearer token:\", access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f886124-0d73-4791-a6b4-dde93e6c3a0d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-06T13:24:59.3162569Z",
       "execution_start_time": "2026-02-06T13:24:57.7870828Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "875b2c11-01f7-42cb-9f0a-3667c6e46a05",
       "queued_time": "2026-02-06T13:24:57.7859383Z",
       "session_id": "70648f3f-687c-463f-a292-3bf7ebb83fd9",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 10,
       "statement_ids": [
        10
       ]
      },
      "text/plain": [
       "StatementMeta(, 70648f3f-687c-463f-a292-3bf7ebb83fd9, 10, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='demo-bike-sales.asazure.windows.net', port=443): Max retries exceeded with url: /xmla?role=Admin (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x722c16aaec50>: Failed to resolve 'demo-bike-sales.asazure.windows.net' ([Errno -2] Name or service not known)\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/urllib3/connection.py:203\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     sock \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[1;32m    204\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dns_host, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport),\n\u001b[1;32m    205\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[1;32m    206\u001b[0m         source_address\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_address,\n\u001b[1;32m    207\u001b[0m         socket_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket_options,\n\u001b[1;32m    208\u001b[0m     )\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/urllib3/util/connection.py:60\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LocationParseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgetaddrinfo(host, port, family, socket\u001b[38;5;241m.\u001b[39mSOCK_STREAM):\n\u001b[1;32m     61\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/socket.py:962\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    961\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 962\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m _socket\u001b[38;5;241m.\u001b[39mgetaddrinfo(host, port, family, \u001b[38;5;28mtype\u001b[39m, proto, flags):\n\u001b[1;32m    963\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno -2] Name or service not known",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNameResolutionError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/urllib3/connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    791\u001b[0m     conn,\n\u001b[1;32m    792\u001b[0m     method,\n\u001b[1;32m    793\u001b[0m     url,\n\u001b[1;32m    794\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    795\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    796\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    797\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    798\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[1;32m    799\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[1;32m    800\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[1;32m    801\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[1;32m    802\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[1;32m    803\u001b[0m )\n\u001b[1;32m    805\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/urllib3/connectionpool.py:491\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    490\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[0;32m--> 491\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[1;32m    493\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/urllib3/connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 467\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_conn(conn)\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/urllib3/connectionpool.py:1096\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1096\u001b[0m     conn\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified:\n",
      "File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/urllib3/connection.py:611\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    610\u001b[0m sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[0;32m--> 611\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_conn()\n\u001b[1;32m    612\u001b[0m server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/urllib3/connection.py:210\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mNameResolutionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x722c16aaec50>: Failed to resolve 'demo-bike-sales.asazure.windows.net' ([Errno -2] Name or service not known)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/urllib3/connectionpool.py:844\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    842\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 844\u001b[0m retries \u001b[38;5;241m=\u001b[39m retries\u001b[38;5;241m.\u001b[39mincrement(\n\u001b[1;32m    845\u001b[0m     method, url, error\u001b[38;5;241m=\u001b[39mnew_e, _pool\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, _stacktrace\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    846\u001b[0m )\n\u001b[1;32m    847\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/urllib3/util/retry.py:515\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    514\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 515\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    517\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='demo-bike-sales.asazure.windows.net', port=443): Max retries exceeded with url: /xmla?role=Admin (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x722c16aaec50>: Failed to resolve 'demo-bike-sales.asazure.windows.net' ([Errno -2] Name or service not known)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m table_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproducts\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Push Spark metadata to semantic model\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m update_semantic_model_from_spark(df, xmla_url, bearer_token, database_name, table_name)\n",
      "Cell \u001b[0;32mIn[26], line 73\u001b[0m, in \u001b[0;36mupdate_semantic_model_from_spark\u001b[0;34m(df, xmla_url, bearer_token, database_name, table_name)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# Full XMLA request\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     xmla_payload \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m<?xml version=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?>\u001b[39m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124m<Envelope xmlns=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://schemas.xmlsoap.org/soap/envelope/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>\u001b[39m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124m  <Body>\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124m</Envelope>\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 73\u001b[0m     response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(xmla_url, headers\u001b[38;5;241m=\u001b[39mheaders, data\u001b[38;5;241m=\u001b[39mxmla_payload)\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to update table \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, data\u001b[38;5;241m=\u001b[39mdata, json\u001b[38;5;241m=\u001b[39mjson, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/requests/adapters.py:519\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    516\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    517\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='demo-bike-sales.asazure.windows.net', port=443): Max retries exceeded with url: /xmla?role=Admin (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x722c16aaec50>: Failed to resolve 'demo-bike-sales.asazure.windows.net' ([Errno -2] Name or service not known)\"))"
     ]
    }
   ],
   "source": [
    "# Load Spark DataFrame\n",
    "df = spark.table(\"products\")\n",
    "\n",
    "# XMLA endpoint (Admin role)\n",
    "xmla_url = f\"https://{WORKSPACE_NAME}.asazure.windows.net/xmla?role=Admin\"\n",
    "# xmla_url = f\"https://asazure.windows.net/xmla?role=Admin&workspaceId=6f4189fd-1e9b-4b03-b188-66b54ecf0c9b\"\n",
    "\n",
    "\n",
    "# Azure AD token\n",
    "bearer_token = access_token\n",
    "\n",
    "# Semantic model (dataset) and table\n",
    "database_name = \"test\"\n",
    "table_name = \"products\"\n",
    "\n",
    "# Push Spark metadata to semantic model\n",
    "update_semantic_model_from_spark(df, xmla_url, bearer_token, database_name, table_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fc9bf87-3126-43c9-8501-6bb90f214094",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-04T12:29:32.7297814Z",
       "execution_start_time": "2026-02-04T12:28:32.2084539Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "d792662b-a616-42f1-b4ef-8f157253ba70",
       "queued_time": "2026-02-04T12:28:32.2073134Z",
       "session_id": "e6cf73bc-46af-4134-b40e-760563cc657c",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 25,
       "statement_ids": [
        25
       ]
      },
      "text/plain": [
       "StatementMeta(, e6cf73bc-46af-4134-b40e-760563cc657c, 25, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"TRUNCATE TABLE customer_addresses\")\n",
    "spark.sql(\"TRUNCATE TABLE customers\")\n",
    "spark.sql(\"TRUNCATE TABLE employee_addresses\")\n",
    "spark.sql(\"TRUNCATE TABLE employees\")\n",
    "spark.sql(\"TRUNCATE TABLE product_categories\")\n",
    "spark.sql(\"TRUNCATE TABLE product_category_texts\")\n",
    "spark.sql(\"TRUNCATE TABLE product_texts\")\n",
    "spark.sql(\"TRUNCATE TABLE products\")\n",
    "spark.sql(\"TRUNCATE TABLE sales_order_items\")\n",
    "spark.sql(\"TRUNCATE TABLE sales_orders\")\n",
    "spark.sql(\"TRUNCATE TABLE vendor_addresses\")\n",
    "spark.sql(\"TRUNCATE TABLE vendors\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "6f4189fd-1e9b-4b03-b188-66b54ecf0c9b",
    "default_lakehouse_name": "LH_Bikes",
    "default_lakehouse_workspace_id": "1a7264cf-5a08-47ab-aa00-41a6f005f2d3",
    "known_lakehouses": [
     {
      "id": "6f4189fd-1e9b-4b03-b188-66b54ecf0c9b"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {
    "29da78fc-7378-411a-b6dc-61a981a69e0c": {
     "persist_state": {
      "view": {
       "chartOptions": {
        "aggregationType": "sum",
        "binsNumber": 10,
        "categoryFieldKeys": [],
        "chartType": "bar",
        "isStacked": false,
        "seriesFieldKeys": [],
        "wordFrequency": "-1"
       },
       "tableOptions": {},
       "type": "details",
       "viewOptionsGroup": [
        {
         "tabItems": [
          {
           "key": "0",
           "name": "Table",
           "options": {},
           "type": "table"
          }
         ]
        }
       ]
      }
     },
     "sync_state": {
      "isSummary": false,
      "language": "scala",
      "table": {
       "rows": [],
       "schema": [
        {
         "key": "0",
         "name": "address_id",
         "type": "int"
        },
        {
         "key": "1",
         "name": "city",
         "type": "string"
        },
        {
         "key": "2",
         "name": "postal_code",
         "type": "string"
        },
        {
         "key": "3",
         "name": "street",
         "type": "string"
        },
        {
         "key": "4",
         "name": "building",
         "type": "string"
        },
        {
         "key": "5",
         "name": "country",
         "type": "string"
        },
        {
         "key": "6",
         "name": "region",
         "type": "string"
        },
        {
         "key": "7",
         "name": "address_type",
         "type": "string"
        },
        {
         "key": "8",
         "name": "validity_start_date",
         "type": "date"
        },
        {
         "key": "9",
         "name": "validity_end_date",
         "type": "date"
        },
        {
         "key": "10",
         "name": "latitude",
         "type": "double"
        },
        {
         "key": "11",
         "name": "longitude",
         "type": "double"
        }
       ],
       "truncated": false
      },
      "wranglerEntryContext": {
       "candidateVariableNames": [
        "addresses_with_missing_customers_df"
       ],
       "dataframeType": "pyspark"
      }
     },
     "type": "Synapse.DataFrame"
    },
    "2aca9896-f7f5-4827-98df-ebfd6e0e1156": {
     "persist_state": {
      "view": {
       "chartOptions": {
        "aggregationType": "sum",
        "binsNumber": 10,
        "categoryFieldKeys": [],
        "chartType": "bar",
        "isStacked": false,
        "seriesFieldKeys": [],
        "wordFrequency": "-1"
       },
       "tableOptions": {},
       "type": "details",
       "viewOptionsGroup": [
        {
         "tabItems": [
          {
           "key": "0",
           "name": "Table",
           "options": {},
           "type": "table"
          }
         ]
        }
       ]
      }
     },
     "sync_state": {
      "isSummary": false,
      "language": "scala",
      "table": {
       "rows": [],
       "schema": [
        {
         "key": "0",
         "name": "address_id",
         "type": "int"
        },
        {
         "key": "1",
         "name": "city",
         "type": "string"
        },
        {
         "key": "2",
         "name": "postal_code",
         "type": "string"
        },
        {
         "key": "3",
         "name": "street",
         "type": "string"
        },
        {
         "key": "4",
         "name": "building",
         "type": "string"
        },
        {
         "key": "5",
         "name": "country",
         "type": "string"
        },
        {
         "key": "6",
         "name": "region",
         "type": "string"
        },
        {
         "key": "7",
         "name": "address_type",
         "type": "string"
        },
        {
         "key": "8",
         "name": "validity_start_date",
         "type": "date"
        },
        {
         "key": "9",
         "name": "validity_end_date",
         "type": "date"
        },
        {
         "key": "10",
         "name": "latitude",
         "type": "double"
        },
        {
         "key": "11",
         "name": "longitude",
         "type": "double"
        }
       ],
       "truncated": false
      },
      "wranglerEntryContext": {
       "candidateVariableNames": [
        "addresses_with_missing_vendors_df"
       ],
       "dataframeType": "pyspark"
      }
     },
     "type": "Synapse.DataFrame"
    },
    "3ec29633-6862-4e4a-9617-a0e60f4b784c": {
     "persist_state": {
      "view": {
       "chartOptions": {
        "aggregationType": "sum",
        "binsNumber": 10,
        "categoryFieldKeys": [],
        "chartType": "bar",
        "isStacked": false,
        "seriesFieldKeys": [],
        "wordFrequency": "-1"
       },
       "tableOptions": {},
       "type": "details",
       "viewOptionsGroup": [
        {
         "tabItems": [
          {
           "key": "0",
           "name": "Table",
           "options": {},
           "type": "table"
          }
         ]
        }
       ]
      }
     },
     "sync_state": {
      "isSummary": false,
      "language": "scala",
      "table": {
       "rows": [],
       "schema": [
        {
         "key": "0",
         "name": "address_id",
         "type": "int"
        },
        {
         "key": "1",
         "name": "city",
         "type": "string"
        },
        {
         "key": "2",
         "name": "postal_code",
         "type": "string"
        },
        {
         "key": "3",
         "name": "street",
         "type": "string"
        },
        {
         "key": "4",
         "name": "building",
         "type": "string"
        },
        {
         "key": "5",
         "name": "country",
         "type": "string"
        },
        {
         "key": "6",
         "name": "region",
         "type": "string"
        },
        {
         "key": "7",
         "name": "address_type",
         "type": "string"
        },
        {
         "key": "8",
         "name": "validity_start_date",
         "type": "date"
        },
        {
         "key": "9",
         "name": "validity_end_date",
         "type": "date"
        },
        {
         "key": "10",
         "name": "latitude",
         "type": "double"
        },
        {
         "key": "11",
         "name": "longitude",
         "type": "double"
        }
       ],
       "truncated": false
      },
      "wranglerEntryContext": {
       "candidateVariableNames": [
        "addresses_with_missing_employees_df"
       ],
       "dataframeType": "pyspark"
      }
     },
     "type": "Synapse.DataFrame"
    },
    "7e28ccc5-78fe-49ba-b607-97e0bbd62ae3": {
     "persist_state": {
      "view": {
       "chartOptions": {
        "aggregationType": "sum",
        "binsNumber": 10,
        "categoryFieldKeys": [],
        "chartType": "bar",
        "isStacked": false,
        "seriesFieldKeys": [],
        "wordFrequency": "-1"
       },
       "tableOptions": {},
       "type": "details",
       "viewOptionsGroup": [
        {
         "tabItems": [
          {
           "key": "0",
           "name": "Table",
           "options": {},
           "type": "table"
          }
         ]
        }
       ]
      }
     },
     "sync_state": {
      "isSummary": false,
      "language": "scala",
      "table": {
       "rows": [],
       "schema": [
        {
         "key": "0",
         "name": "vendor_id",
         "type": "int"
        },
        {
         "key": "1",
         "name": "email",
         "type": "string"
        },
        {
         "key": "2",
         "name": "phone_no",
         "type": "string"
        },
        {
         "key": "3",
         "name": "fax_no",
         "type": "string"
        },
        {
         "key": "4",
         "name": "url",
         "type": "string"
        },
        {
         "key": "5",
         "name": "address_id",
         "type": "int"
        },
        {
         "key": "6",
         "name": "company_name",
         "type": "string"
        },
        {
         "key": "7",
         "name": "legal_form",
         "type": "string"
        },
        {
         "key": "8",
         "name": "created_by",
         "type": "int"
        },
        {
         "key": "9",
         "name": "created_date",
         "type": "date"
        },
        {
         "key": "10",
         "name": "modified_by",
         "type": "int"
        },
        {
         "key": "11",
         "name": "modified_date",
         "type": "date"
        },
        {
         "key": "12",
         "name": "currency",
         "type": "string"
        }
       ],
       "truncated": false
      },
      "wranglerEntryContext": {
       "candidateVariableNames": [
        "employees_with_missing_addresses_df"
       ],
       "dataframeType": "pyspark"
      }
     },
     "type": "Synapse.DataFrame"
    },
    "8393ed1e-0689-40a2-b0e9-c0af8bab768e": {
     "persist_state": {
      "view": {
       "chartOptions": {
        "aggregationType": "sum",
        "binsNumber": 10,
        "categoryFieldKeys": [],
        "chartType": "bar",
        "isStacked": false,
        "seriesFieldKeys": [],
        "wordFrequency": "-1"
       },
       "tableOptions": {},
       "type": "details",
       "viewOptionsGroup": [
        {
         "tabItems": [
          {
           "key": "0",
           "name": "Table",
           "options": {},
           "type": "table"
          }
         ]
        }
       ]
      }
     },
     "sync_state": {
      "isSummary": false,
      "language": "scala",
      "table": {
       "rows": [],
       "schema": [
        {
         "key": "0",
         "name": "vendor_id",
         "type": "int"
        },
        {
         "key": "1",
         "name": "email",
         "type": "string"
        },
        {
         "key": "2",
         "name": "phone_no",
         "type": "string"
        },
        {
         "key": "3",
         "name": "fax_no",
         "type": "string"
        },
        {
         "key": "4",
         "name": "url",
         "type": "string"
        },
        {
         "key": "5",
         "name": "address_id",
         "type": "int"
        },
        {
         "key": "6",
         "name": "company_name",
         "type": "string"
        },
        {
         "key": "7",
         "name": "legal_form",
         "type": "string"
        },
        {
         "key": "8",
         "name": "created_by",
         "type": "int"
        },
        {
         "key": "9",
         "name": "created_date",
         "type": "date"
        },
        {
         "key": "10",
         "name": "modified_by",
         "type": "int"
        },
        {
         "key": "11",
         "name": "modified_date",
         "type": "date"
        },
        {
         "key": "12",
         "name": "currency",
         "type": "string"
        }
       ],
       "truncated": false
      },
      "wranglerEntryContext": {
       "candidateVariableNames": [
        "vendors_with_missing_addresses_df"
       ],
       "dataframeType": "pyspark"
      }
     },
     "type": "Synapse.DataFrame"
    },
    "cfc1b496-1dca-48ee-be3f-f15d7d4d3ae9": {
     "persist_state": {
      "view": {
       "chartOptions": {
        "aggregationType": "sum",
        "binsNumber": 10,
        "categoryFieldKeys": [],
        "chartType": "bar",
        "isStacked": false,
        "seriesFieldKeys": [],
        "wordFrequency": "-1"
       },
       "tableOptions": {},
       "type": "details",
       "viewOptionsGroup": [
        {
         "tabItems": [
          {
           "key": "0",
           "name": "Table",
           "options": {},
           "type": "table"
          }
         ]
        }
       ]
      }
     },
     "sync_state": {
      "isSummary": false,
      "language": "scala",
      "table": {
       "rows": [],
       "schema": [
        {
         "key": "0",
         "name": "vendor_id",
         "type": "int"
        },
        {
         "key": "1",
         "name": "email",
         "type": "string"
        },
        {
         "key": "2",
         "name": "phone_no",
         "type": "string"
        },
        {
         "key": "3",
         "name": "fax_no",
         "type": "string"
        },
        {
         "key": "4",
         "name": "url",
         "type": "string"
        },
        {
         "key": "5",
         "name": "address_id",
         "type": "int"
        },
        {
         "key": "6",
         "name": "company_name",
         "type": "string"
        },
        {
         "key": "7",
         "name": "legal_form",
         "type": "string"
        },
        {
         "key": "8",
         "name": "created_by",
         "type": "int"
        },
        {
         "key": "9",
         "name": "created_date",
         "type": "date"
        },
        {
         "key": "10",
         "name": "modified_by",
         "type": "int"
        },
        {
         "key": "11",
         "name": "modified_date",
         "type": "date"
        },
        {
         "key": "12",
         "name": "currency",
         "type": "string"
        }
       ],
       "truncated": false
      },
      "wranglerEntryContext": {
       "candidateVariableNames": [
        "customers_with_missing_addresses_df"
       ],
       "dataframeType": "pyspark"
      }
     },
     "type": "Synapse.DataFrame"
    }
   },
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
